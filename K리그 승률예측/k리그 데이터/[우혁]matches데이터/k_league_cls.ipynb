{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from matplotlib import font_manager, rc\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "# from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, PowerTransformer, OrdinalEncoder,\n",
    "    OneHotEncoder, FunctionTransformer, PolynomialFeatures, LabelEncoder, MinMaxScaler\n",
    ")\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, RFE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression, LinearRegression, Ridge, Lasso,\n",
    "    SGDRegressor, ElasticNet\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, cross_validate,\n",
    "    GridSearchCV, KFold, cross_val_predict\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, mean_squared_error, make_scorer, accuracy_score, log_loss\n",
    ")\n",
    "from sklearn import set_config, datasets\n",
    "from catboost import (\n",
    "    CatBoostRegressor, CatBoostClassifier,\n",
    ")\n",
    "# import category_encoders as ce\n",
    "# from sklearn.pipeline import (\n",
    "#     Pipeline, FeatureUnion, make_pipeline\n",
    "# )\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, StackingClassifier, StackingRegressor,\n",
    "    GradientBoostingRegressor, VotingClassifier, VotingRegressor,\n",
    "    HistGradientBoostingRegressor, GradientBoostingClassifier,\n",
    "    BaggingClassifier, AdaBoostClassifier, RandomForestRegressor,ExtraTreesRegressor\n",
    ")\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import math\n",
    "import optuna\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('matches_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2480, 29)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = df.loc[(df['home_team_goal_count'] != 0) & (df['home_team_result'] == '승')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'home_team_result' 무 -> 승,패로 업데이트\n",
    "df.loc[(df['home_team_result'] == '무') & (df['home_team_shots_on_target'] >= df['away_team_shots_on_target']), 'home_team_result'] = '승'\n",
    "df.loc[(df['home_team_result'] == '무') & (df['home_team_shots_on_target'] < df['away_team_shots_on_target']), 'home_team_result'] = '패'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature fun\n",
    "def team_encoding(train):\n",
    "    train['home_win'] = train['home_team_result'].apply(lambda x: 1 if x=='승' else 0) # home_win 열 추가, 승리인 경우 1, 아닌 경우 0\n",
    "    dic = {}\n",
    "    # 각 홈팀별 이긴 경기 수를 딕셔너리에 저장\n",
    "    for team in train['home_team_name'].unique():\n",
    "        value = train[train['home_team_name'] == team]['home_win'].sum() \n",
    "        #home_team_name  열에서 고유한 팀 이름을 가져와 각 팀이 홈에서 이긴 경기 수 계산, 이 값을 dic에 저장\n",
    "        dic[team] = value\n",
    "\n",
    "    label_dic={}\n",
    "    # 승리 횧수를 기준으로 오름차순 정렬, 각 팀에 대해 라벨 부여, 승리 횟수가 적은 팀부터 0,1,2 의 라벨을 부여\n",
    "    for idx, (team, _) in enumerate(sorted(dic.items(), key= lambda x: x[1])):\n",
    "        label_dic[team] = idx\n",
    "    \n",
    "    return label_dic\n",
    "\n",
    "\n",
    "''' 홈팀 득점 이동평균 계산 함수 '''\n",
    "\n",
    "def homeGoal_day_mean(train, test, day):\n",
    "    train[f'home_Goal_{day}_mean'] = -1  # 초기값 -1로 설정\n",
    "    test[f'home_Goal_{day}_mean'] = -1\n",
    "    \n",
    "    teams = train['home_team_name'].unique()\n",
    "    for team in tqdm(teams): # train에서 고유 팀 이름을 가져오고 이를 시각적으로 표시해줌 : tqdm\n",
    "        team_df = train[train['home_team_name'] == team]\n",
    "        # 롤링 윈도우 크기 설정\n",
    "        ch_day = len(team_df) if len(team_df) < day else day # 팀의 경기 수가 주어진 day 보다 적으면, 경기 수 만큼의 윈도우 크기 사용\n",
    "        idx = team_df['home_team_goal_count'].rolling(ch_day).mean().index.values # 롤링 윈도우 평균 계산\n",
    "        val = team_df['home_team_goal_count'].rolling(ch_day).mean().values\n",
    "        train[f'home_Goal_{day}_mean'].loc[idx] = val\n",
    "        test_idx = test[test['home_team_name'] == team].index\n",
    "        test[f'home_Goal_{day}_mean'].loc[test_idx] = val[-1]\n",
    "    # 결측값 처리\n",
    "    train[f'home_Goal_{day}_mean'] = train[f'home_Goal_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "''' 원정팀 득점 이동평균 계산 함수 '''\n",
    "\n",
    "def awayGoal_day_mean(train, test, day):\n",
    "    # 초기값 설정\n",
    "    train[f'away_Goal_{day}_mean'] = -1\n",
    "    test[f'away_Goal_{day}_mean'] = -1\n",
    "    \n",
    "    teams = train['away_team_name'].unique()\n",
    "    for team in tqdm(teams):\n",
    "        team_df = train[train['away_team_name'] == team]\n",
    "        # 롤링 윈도우 크기 설정\n",
    "        ch_day = len(team_df) if len(team_df) < day else day\n",
    "        idx = team_df['away_team_goal_count'].rolling(ch_day).mean().index.values\n",
    "        val = team_df['away_team_goal_count'].rolling(ch_day).mean().values\n",
    "        train[f'away_Goal_{day}_mean'].loc[idx] = val\n",
    "        test_idx = test[test['away_team_name'] == team].index\n",
    "        test[f'away_Goal_{day}_mean'].loc[test_idx] = val[-1]\n",
    "    # 결측값 처리\n",
    "    train[f'away_Goal_{day}_mean'] = train[f'away_Goal_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "# '''홈팀 승리율 평균 계산 함수'''\n",
    "\n",
    "# def homeWin_day_mean(train, test, day):\n",
    "#     train[f'home_winRate_{day}_mean'] = -1\n",
    "#     test[f'home_winRate_{day}_mean'] = -1\n",
    "#     train['win'] = train['home_team_result'].apply(lambda x: 1 if x == '승' else 0)\n",
    "\n",
    "#     teams = train['home_team_name'].unique()\n",
    "#     for team in tqdm(teams):\n",
    "#         team_df = train[train['home_team_name'] == team]\n",
    "#         ch_day = len(team_df) if len(team_df) < day else day\n",
    "#         idx = team_df['win'].rolling(ch_day).mean().index.values\n",
    "#         val = team_df['win'].rolling(ch_day).mean().values\n",
    "#         train[f'home_winRate_{day}_mean'].loc[idx] = val\n",
    "#         test_idx = test[test['home_team_name'] == team].index\n",
    "#         test[f'home_winRate_{day}_mean'].loc[test_idx] = val[-1]\n",
    "\n",
    "#     train.drop(columns=['win'], inplace=True)\n",
    "\n",
    "#     train[f'home_winRate_{day}_mean'] = train[f'home_winRate_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "# '''원정팀 승리율 평균 계산 함수'''\n",
    "\n",
    "# def awayWin_day_mean(train, test, day):\n",
    "\n",
    "#     train[f'away_winRate_{day}_mean'] = -1\n",
    "#     test[f'away_winRate_{day}_mean'] = -1\n",
    "#     train['win'] = train['home_team_result'].apply(lambda x: 1 if x == '패' else 0)\n",
    "    \n",
    "#     teams = train['away_team_name'].unique()\n",
    "#     for team in tqdm(teams):\n",
    "#         team_df = train[train['away_team_name'] == team]\n",
    "\n",
    "#         ch_day = len(team_df) if len(team_df) < day else day\n",
    "#         idx = team_df['win'].rolling(ch_day).mean().index.values\n",
    "#         val = team_df['win'].rolling(ch_day).mean().values\n",
    "#         train[f'away_winRate_{day}_mean'].loc[idx] = val\n",
    "#         test_idx = test[test['away_team_name'] == team].index\n",
    "#         test[f'away_winRate_{day}_mean'].loc[test_idx] = val[-1]\n",
    "\n",
    "#     train.drop(columns=['win'], inplace=True)\n",
    "\n",
    "#     train[f'away_winRate_{day}_mean'] = train[f'away_winRate_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "'''홈팀 평균 계산 함수'''\n",
    "\n",
    "def home_day_mean(train, test, columns, day):\n",
    "    for column in tqdm(columns):\n",
    "        teams = train['home_team_name'].values\n",
    "        train[f'home_{column}_{day}_mean'] = -1\n",
    "        test[f'home_{column}_{day}_mean'] = -1\n",
    "\n",
    "        for team in tqdm(teams):\n",
    "            team_df = train[train['home_team_name'] == team]\n",
    "            idx = team_df[column].rolling(day).mean().index.values\n",
    "            val = team_df[column].rolling(day).mean().values\n",
    "            train[f'home_{column}_{day}_mean'].loc[idx] = val\n",
    "            test_idx = test[test['home_team_name'] == team].index\n",
    "            test[f'home_{column}_{day}_mean'].loc[test_idx] = val[-1]\n",
    "\n",
    "        train[f'home_{column}_{day}_mean'] = train[f'home_{column}_{day}_mean'].fillna(0)\n",
    "        test[f'home_{column}_{day}_mean'] = test[f'home_{column}_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "'''원정팀 평균 계산 함수'''\n",
    "\n",
    "def away_day_mean(train, test, columns, day):\n",
    "    for column in tqdm(columns):\n",
    "        teams = train['away_team_name'].values\n",
    "        train[f'away_{column}_{day}_mean'] = -1\n",
    "        test[f'away_{column}_{day}_mean'] = -1\n",
    "\n",
    "        for team in tqdm(teams):\n",
    "            team_df = train[train['away_team_name'] == team]\n",
    "            idx = team_df[column].rolling(day).mean().index.values\n",
    "            val = team_df[column].rolling(day).mean().values\n",
    "            train[f'away_{column}_{day}_mean'].loc[idx] = val\n",
    "            test_idx = test[test['away_team_name'] == team].index\n",
    "            test[f'away_{column}_{day}_mean'].loc[test_idx] = val[-1]\n",
    "\n",
    "        train[f'away_{column}_{day}_mean'] = train[f'away_{column}_{day}_mean'].fillna(0)\n",
    "        test[f'away_{column}_{day}_mean'] = test[f'away_{column}_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "'''전처리 함수'''\n",
    "\n",
    "def preprocessing(train, test):\n",
    "    # 년과 월일로 나누기\n",
    "    train['date_GMT'] = train['date_GMT'].dt.strftime('%Y%m%d')\n",
    "    test['date_GMT'] = test['date_GMT'].dt.strftime('%Y%m%d')\n",
    "    \n",
    "    train['year'] = train['date_GMT'].apply(lambda x : int(x[0:4]))\n",
    "    train['date'] = train['date_GMT'].apply(lambda x : int(x[4:10]))\n",
    "\n",
    "    test['year'] = test['date_GMT'].apply(lambda x : int(x[0:4]))\n",
    "    test['date'] = test['date_GMT'].apply(lambda x : int(x[4:10]))\n",
    "\n",
    "    # train.drop(columns=['date_GMT'], inplace=True)\n",
    "    # test.drop(columns=['date_GMT'], inplace=True)\n",
    "\n",
    "    # # 팀 인코딩 적용   # 위에서 적용 했음\n",
    "    # label_dic = dic\n",
    "    # train['home_team_name'] = train['home_team_name'].apply(lambda x: label_dic[x])\n",
    "    # train['away_team_name'] = train['away_team_name'].apply(lambda x: label_dic[x])\n",
    "    # test['home_team_name'] = test['home_team_name'].apply(lambda x: label_dic[x])\n",
    "    # test['away_team_name'] = test['away_team_name'].apply(lambda x: label_dic[x])\n",
    "\n",
    "    # # 5일간 홈팀 승리 비율 계산    ### 이거 쓰레기인듯\n",
    "    # homeWin_day_mean(train, test, 5)\n",
    "    # # 5일간 원정팀 승리 비율 계산\n",
    "    # awayWin_day_mean(train, test, 5)\n",
    "\n",
    "    # 5일간 홈팀 평균 득점 계산\n",
    "    homeGoal_day_mean(train, test, 5)\n",
    "\n",
    "    # 5일간 원정팀 평균 득점 계산\n",
    "    awayGoal_day_mean(train, test, 5)\n",
    "\n",
    "    # 불필요한 컬럼 제거\n",
    "    train = train.drop(columns=['index','home_team_goal_count','away_team_goal_count','game_points'])\n",
    "    test = test.drop(columns=['index','home_team_goal_count','away_team_goal_count','home_team_result','game_points'])\n",
    "\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y\n",
    "X = df.drop(columns=['away_team_possession'])  \n",
    "y = df['home_team_result']\n",
    "# split_index = 1796 # 2021년도까지의 index\n",
    "# train = X.iloc[:split_index]\n",
    "# test = X.iloc[split_index:]\n",
    "# y_train = y.iloc[:split_index]\n",
    "# y_test = y.iloc[split_index:]\n",
    "\n",
    "# team_name 인코딩\n",
    "cat = ['home_team_name','away_team_name']\n",
    "le = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=int)\n",
    "X[cat] = le.fit_transform(X[cat])\n",
    "\n",
    "# 승무패 인코딩\n",
    "lec = LabelEncoder()\n",
    "lec.fit(X['home_team_result'])\n",
    "y = lec.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:00<00:00, 778.48it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 852.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1984, 28) (496, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train,Test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) ## 이 방법에 의문을 품는 바\n",
    "\n",
    "X_train, X_val= preprocessing(X_train, X_val)\n",
    "X_val_idx = X_val.index.values\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 홈팀과 원정팀의 공격 효율성을 계산한 피쳐 생성\n",
    "X_train['home_attack_efficiency'] = X_train['home_Goal_5_mean'] * X_train['home_team_shots_on_target']\n",
    "X_train['away_attack_efficiency'] = X_train['away_Goal_5_mean'] * X_train['away_team_shots_on_target']\n",
    "\n",
    "# 홈팀과 원정팀의 공격 효율성 차이를 나타내는 피쳐 생성\n",
    "X_train['attack_efficiency_difference'] = X_train['home_attack_efficiency'] - X_train['away_attack_efficiency']\n",
    "\n",
    "# 홈팀과 원정팀의 전반 골 수 차이를 나타내는 피쳐 생성\n",
    "X_train['goal_count_diff'] = X_train['home_team_goal_count_half_time'] - X_train['away_team_goal_count_half_time']\n",
    "\n",
    "# 최근 5경기 평균 득점의 표준 편차를 나타내는 피쳐 생성\n",
    "X_train['home_Goal_5_std'] = X_train['home_Goal_5_mean'].rolling(window=5).std()\n",
    "X_train['away_Goal_5_std'] = X_train['away_Goal_5_mean'].rolling(window=5).std()\n",
    "\n",
    "# 결측값을 0으로 대체\n",
    "X_train = X_train.fillna(0)\n",
    "\n",
    "# 테스트 데이터에도 동일한 피쳐 생성\n",
    "X_val['home_attack_efficiency'] = X_val['home_Goal_5_mean'] * X_val['home_team_shots_on_target']\n",
    "X_val['away_attack_efficiency'] = X_val['away_Goal_5_mean'] * X_val['away_team_shots_on_target']\n",
    "X_val['attack_efficiency_difference'] = X_val['home_attack_efficiency'] - X_val['away_attack_efficiency']\n",
    "X_val['goal_count_diff'] = X_val['home_team_goal_count_half_time'] - X_val['away_team_goal_count_half_time']\n",
    "X_val['home_Goal_5_std'] = X_val['home_Goal_5_mean'].rolling(window=5).std()\n",
    "X_val['away_Goal_5_std'] = X_val['away_Goal_5_mean'].rolling(window=5).std()\n",
    "X_val = X_val.fillna(0)\n",
    "\n",
    "# 학습 데이터에서 목표 변수 'home_team_result' 컬럼 제거\n",
    "X_train.drop(columns = ['home_team_result'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler\n",
    "cat = ['home_team_name','away_team_name']\n",
    "\n",
    "num_features = list(set(X_train.columns) - set(cat))\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "X_train[num_features] = scaler.fit_transform(X_train[num_features])\n",
    "X_val[num_features] = scaler.transform(X_val[num_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled class distribution: Counter({0: 100000, 1: 100000})\n",
      "Original training set shape: (1984, 33)\n",
      "Resampled training set shape: (200000, 33)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# SMOTE 객체 생성 (각 클래스의 샘플 수를 1000개로 설정)\n",
    "smote = SMOTE(sampling_strategy={0: 100000, 1: 100000}, random_state=42)\n",
    "\n",
    "# SMOTE-Tomek 객체 생성\n",
    "smote_tomek = SMOTETomek(smote=smote, random_state=42)\n",
    "\n",
    "# 오버샘플링 및 언더샘플링 적용\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# 각 클래스 비율 확인\n",
    "print(f\"Resampled class distribution: {Counter(y_train_resampled)}\")\n",
    "print(f\"Original training set shape: {X_train.shape}\")\n",
    "print(f\"Resampled training set shape: {X_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pycaret.classification import *\n",
    "\n",
    "# setup_clf = setup(data = X_train, target = y_train, session_id = 42)\n",
    "# model = compare_models(sort = 'Accuracy', fold = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_tune = tune_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model(best_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import xgboost as xgb\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "# import shap\n",
    "# SHAP_THRESHOLD = 0\n",
    "\n",
    "# # feature_names dimension 조정\n",
    "# X_train_col = X_train.columns\n",
    "# feature_names = X_train_col.to_numpy()\n",
    "\n",
    "# # 모델 학습\n",
    "# model = xgb.XGBClassifier().fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# # 모델 예측 및 평가\n",
    "# y_pred = model.predict(X_val)\n",
    "# print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "# # SHAP 값 계산\n",
    "# explainer = shap.TreeExplainer(model)\n",
    "# shap_values = explainer.shap_values(X_val)\n",
    "\n",
    "# # SHAP 값 요약\n",
    "# if isinstance(shap_values, list):  # shap_values가 리스트일 경우 (XGBoost >= 1.0.0)\n",
    "#     shap_values = shap_values[1]\n",
    "\n",
    "# shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "# importance_df = pd.DataFrame({'column_name': feature_names, 'shap_importance': shap_sum})\n",
    "# importance_df = importance_df.sort_values('shap_importance', ascending=False)\n",
    "\n",
    "# # 중요도 임계값 적용 (선택 사항)\n",
    "# importance_df_filtered = importance_df[importance_df['shap_importance'] > SHAP_THRESHOLD]\n",
    "# print(\"Filtered SHAP Importances:\\n\", importance_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 지정된(SHAP_THRESHOLD) Shap feature 중요도 이상인 것만 선택\n",
    "# features_selected = importance_df.query('shap_importance > @SHAP_THRESHOLD').column_name.tolist()\n",
    "# shap_xgb_X_train_resampled = X_train_resampled[features_selected]\n",
    "# shap_xgb_X_val = X_val[features_selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LightGBM 모델 학습\n",
    "# model = lgb.LGBMClassifier().fit(X_train_resampled, y_train_resampled)\n",
    "# SHAP_THRESHOLD = 0.3\n",
    "\n",
    "# # 모델 예측 및 평가\n",
    "# y_pred = model.predict(X_val)\n",
    "# print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "# # SHAP 값 계산\n",
    "# explainer = shap.TreeExplainer(model)\n",
    "# shap_values = explainer.shap_values(X_val)\n",
    "\n",
    "# shap_sum = np.abs(shap_values).mean(axis=1)[1,:]\n",
    "# importance_df = pd.DataFrame([X_val.columns.tolist(), shap_sum.tolist()]).T\n",
    "# importance_df.columns = ['column_name', 'shap_importance']\n",
    "# importance_df = importance_df.sort_values('shap_importance', ascending=False);\n",
    "# importance_df\n",
    "\n",
    "# # SHAP 값 데이터프레임 생성 (각 피쳐별 SHAP 값)\n",
    "# shap_values_df = pd.DataFrame(shap_values[1], columns=X_val.columns)\n",
    "# shap_values_df\n",
    "\n",
    "# # SHAP 값의 평균 절대값 계산\n",
    "# shap_abs_mean = pd.DataFrame(shap_values[1], columns=X_val.columns).abs().mean().sort_values(ascending=False)\n",
    "\n",
    "# # SHAP 값 평균 절대값 시각화\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# shap_abs_mean.plot(kind='barh')\n",
    "# plt.title(\"Mean Absolute SHAP Values for Features\")\n",
    "# plt.xlabel(\"Mean Absolute SHAP Value\")\n",
    "# plt.ylabel(\"Features\")\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 지정된(SHAP_THRESHOLD) Shap feature 중요도 이상인 것만 선택\n",
    "# features_selected = importance_df.query('shap_importance > @SHAP_THRESHOLD').column_name.tolist()\n",
    "# shap_lgbm_X_train_resampled = X_train_resampled[features_selected]\n",
    "# shap_lgbm_X_val = X_val[features_selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model Oputna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # xgboostclassifier\n",
    "# import optuna\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# def xgb_objective(trial):\n",
    "#     # 하이퍼파라미터 범위 설정\n",
    "#     max_depth = trial.suggest_int('max_depth', 3, 7)  # max_depth의 범위를 줄임\n",
    "#     learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "#     n_estimators = trial.suggest_int('n_estimators', 50, 200)  # n_estimators의 범위를 줄임\n",
    "#     subsample = trial.suggest_float('subsample', 0.5, 0.9)\n",
    "#     colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 0.9)\n",
    "#     min_child_weight = trial.suggest_int('min_child_weight', 4, 10)  # 추가\n",
    "#     gamma = trial.suggest_float('gamma', 0, 5)  # 추가\n",
    "    \n",
    "#     # XGBClassifier 모델 정의\n",
    "#     clf = XGBClassifier(\n",
    "#         max_depth=max_depth,\n",
    "#         learning_rate=learning_rate,\n",
    "#         n_estimators=n_estimators,\n",
    "#         subsample=subsample,\n",
    "#         colsample_bytree=colsample_bytree,\n",
    "#         min_child_weight=min_child_weight,  # 추가\n",
    "#         gamma=gamma,  # 추가\n",
    "#         use_label_encoder=False,\n",
    "#         eval_metric='logloss'\n",
    "#     )\n",
    "    \n",
    "#     # 교차 검증 점수 계산\n",
    "#     scores = cross_val_score(clf, shap_xgb_X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "    \n",
    "#     return scores.mean()\n",
    "\n",
    "# # Optuna 스터디 생성 및 최적화\n",
    "# xgb_study = optuna.create_study(direction='maximize')\n",
    "# xgb_study.optimize(xgb_objective, n_trials=50)  # 최적화 반복 횟수는 필요에 따라 조절\n",
    "\n",
    "# # 최적 하이퍼파라미터 출력\n",
    "# xgb_best_params = xgb_study.best_params\n",
    "# print(' ')\n",
    "# print(xgb_study.best_value)\n",
    "# print(xgb_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_logis=xgb.XGBClassifier(**xgb_best_params)\n",
    "# # model_logis=xgb.XGBClassifier(max_depth= 4, learning_rate= 0.00020353095689003422, n_estimators= 229, subsample= 0.5217582675029752, colsample_bytree= 0.5421399627551824)\n",
    "# model_logis.fit(shap_xgb_X_train_resampled, y_train_resampled)\n",
    "# y_pred = model_logis.predict(shap_xgb_X_val)\n",
    "\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(\"Accuracy=\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LogisticRegression\n",
    "# def logreg_objective(trial):\n",
    "\n",
    "#     r = trial.suggest_float('l1_ratio', 0.3, 0.8, log=False)  # 범위를 0.1에서 0.9로 좁힘\n",
    "#     c = trial.suggest_float('C', 1e-4, 1e2, log=True)\n",
    "#     max_iter = trial.suggest_int('max_iter', 500, 2000, step=500)  # max_iter 튜닝 추가\n",
    "    \n",
    "#     clf =  LogisticRegression(max_iter=max_iter, solver='saga', penalty='elasticnet', l1_ratio=r, C=c)\n",
    "#     scores = cross_val_score(clf, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "    \n",
    "#     return scores.mean()\n",
    "    \n",
    "# logreg_study = optuna.create_study(direction='maximize')\n",
    "# logreg_study.optimize(logreg_objective, n_trials=20)\n",
    "\n",
    "# logreg_best_params = logreg_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_logis=LogisticRegression(**logreg_best_params)\n",
    "# model_logis.fit(X_train_resampled,y_train_resampled)\n",
    "# y_pred = model_logis.predict(X_val)\n",
    "\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-09 21:12:21,772] A new study created in memory with name: no-name-43949888-f47a-4544-9631-76d9c98c96fe\n",
      "[I 2024-07-09 21:12:23,399] Trial 0 finished with value: 0.9421702919999999 and parameters: {'alpha': 1.797036628666591e-05}. Best is trial 0 with value: 0.9421702919999999.\n",
      "[I 2024-07-09 21:12:25,007] Trial 1 finished with value: 0.9423530550000001 and parameters: {'alpha': 817.8400784422937}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:26,597] Trial 2 finished with value: 0.942138422 and parameters: {'alpha': 19.2544655444134}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:28,157] Trial 3 finished with value: 0.9421305134999999 and parameters: {'alpha': 1.265930989012963}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:29,708] Trial 4 finished with value: 0.9422654364999999 and parameters: {'alpha': 386.18817659882325}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:31,267] Trial 5 finished with value: 0.942213784 and parameters: {'alpha': 215.21324325553547}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:32,837] Trial 6 finished with value: 0.942130204 and parameters: {'alpha': 0.6133926647280473}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:34,385] Trial 7 finished with value: 0.9421355825 and parameters: {'alpha': 0.0019137749871797859}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:35,937] Trial 8 finished with value: 0.9421354770000001 and parameters: {'alpha': 12.575980635905603}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:37,510] Trial 9 finished with value: 0.9423488499999999 and parameters: {'alpha': 789.3904335643978}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:39,087] Trial 10 finished with value: 0.9421313124999999 and parameters: {'alpha': 0.00890070818889045}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:40,649] Trial 11 finished with value: 0.9423369239999999 and parameters: {'alpha': 715.3802677518239}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:42,250] Trial 12 finished with value: 0.942136318 and parameters: {'alpha': 14.442386507475614}. Best is trial 1 with value: 0.9423530550000001.\n",
      "[I 2024-07-09 21:12:43,830] Trial 13 finished with value: 0.9423601235 and parameters: {'alpha': 869.4263638721119}. Best is trial 13 with value: 0.9423601235.\n",
      "[I 2024-07-09 21:12:45,388] Trial 14 finished with value: 0.9421461924999999 and parameters: {'alpha': 37.11260721618222}. Best is trial 13 with value: 0.9423601235.\n",
      "[I 2024-07-09 21:12:46,958] Trial 15 finished with value: 0.942130076 and parameters: {'alpha': 0.11646951791292723}. Best is trial 13 with value: 0.9423601235.\n",
      "[I 2024-07-09 21:12:48,543] Trial 16 finished with value: 0.9421657509999999 and parameters: {'alpha': 8.590332894928386e-05}. Best is trial 13 with value: 0.9423601235.\n",
      "[I 2024-07-09 21:12:50,108] Trial 17 finished with value: 0.9421303059999999 and parameters: {'alpha': 0.8855674139741635}. Best is trial 13 with value: 0.9423601235.\n",
      "[I 2024-07-09 21:12:51,678] Trial 18 finished with value: 0.9421575004999999 and parameters: {'alpha': 64.02053881264641}. Best is trial 13 with value: 0.9423601235.\n",
      "[I 2024-07-09 21:12:53,248] Trial 19 finished with value: 0.9421311540000001 and parameters: {'alpha': 2.7767364323229673}. Best is trial 13 with value: 0.9423601235.\n",
      "[I 2024-07-09 21:12:54,842] Trial 20 finished with value: 0.942133231 and parameters: {'alpha': 0.0035561947771568844}. Best is trial 13 with value: 0.9423601235.\n",
      "[I 2024-07-09 21:12:56,410] Trial 21 finished with value: 0.9423570235 and parameters: {'alpha': 845.8264209035798}. Best is trial 13 with value: 0.9423601235.\n",
      "[I 2024-07-09 21:12:57,970] Trial 22 finished with value: 0.9421818175000001 and parameters: {'alpha': 126.40600948795252}. Best is trial 13 with value: 0.9423601235.\n",
      "[I 2024-07-09 21:12:59,548] Trial 23 finished with value: 0.9421822040000001 and parameters: {'alpha': 127.55988981649493}. Best is trial 13 with value: 0.9423601235.\n",
      "[I 2024-07-09 21:13:01,140] Trial 24 finished with value: 0.9423658825000001 and parameters: {'alpha': 916.6411611638317}. Best is trial 24 with value: 0.9423658825000001.\n",
      "[I 2024-07-09 21:13:02,715] Trial 25 finished with value: 0.9421312315 and parameters: {'alpha': 2.982795237791362}. Best is trial 24 with value: 0.9423658825000001.\n",
      "[I 2024-07-09 21:13:04,272] Trial 26 finished with value: 0.9421301395 and parameters: {'alpha': 0.05590509991986321}. Best is trial 24 with value: 0.9423658825000001.\n",
      "[I 2024-07-09 21:13:05,860] Trial 27 finished with value: 0.9421604834999998 and parameters: {'alpha': 71.70597247064619}. Best is trial 24 with value: 0.9423658825000001.\n",
      "[I 2024-07-09 21:13:07,429] Trial 28 finished with value: 0.94223508 and parameters: {'alpha': 280.7280041589446}. Best is trial 24 with value: 0.9423658825000001.\n",
      "[I 2024-07-09 21:13:08,988] Trial 29 finished with value: 0.9421706175000001 and parameters: {'alpha': 1.1222248728025843e-05}. Best is trial 24 with value: 0.9423658825000001.\n",
      "[I 2024-07-09 21:13:10,568] Trial 30 finished with value: 0.9421646515 and parameters: {'alpha': 0.00010029253442640296}. Best is trial 24 with value: 0.9423658825000001.\n",
      "[I 2024-07-09 21:13:12,152] Trial 31 finished with value: 0.9423452035000001 and parameters: {'alpha': 767.1583733312845}. Best is trial 24 with value: 0.9423658825000001.\n",
      "[I 2024-07-09 21:13:13,738] Trial 32 finished with value: 0.9423750314999999 and parameters: {'alpha': 997.017594942745}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:15,303] Trial 33 finished with value: 0.942370977 and parameters: {'alpha': 960.3791622252487}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:16,920] Trial 34 finished with value: 0.9421445455 and parameters: {'alpha': 33.39556338046026}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:18,489] Trial 35 finished with value: 0.9422284695 and parameters: {'alpha': 259.30858801631547}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:20,069] Trial 36 finished with value: 0.9421329654999999 and parameters: {'alpha': 7.057864100425182}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:21,641] Trial 37 finished with value: 0.9422184695 and parameters: {'alpha': 229.35564029081763}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:23,201] Trial 38 finished with value: 0.9421616835 and parameters: {'alpha': 74.77012104474414}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:24,790] Trial 39 finished with value: 0.9422532594999999 and parameters: {'alpha': 341.5063188847073}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:26,339] Trial 40 finished with value: 0.9421327085 and parameters: {'alpha': 6.529536567191364}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:27,923] Trial 41 finished with value: 0.942372705 and parameters: {'alpha': 975.564745281128}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:29,483] Trial 42 finished with value: 0.942357984 and parameters: {'alpha': 853.0025828319098}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:31,062] Trial 43 finished with value: 0.94219011 and parameters: {'alpha': 148.78599201531162}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:32,640] Trial 44 finished with value: 0.9422838139999999 and parameters: {'alpha': 456.53899931177835}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:34,221] Trial 45 finished with value: 0.9421401724999999 and parameters: {'alpha': 23.13916454402185}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:35,793] Trial 46 finished with value: 0.9422771285 and parameters: {'alpha': 429.34959951559466}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:37,360] Trial 47 finished with value: 0.9421612829999999 and parameters: {'alpha': 73.8189253988955}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:38,937] Trial 48 finished with value: 0.9421944595000001 and parameters: {'alpha': 160.17555768062965}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:40,506] Trial 49 finished with value: 0.9421301005 and parameters: {'alpha': 0.20295783471812923}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:42,091] Trial 50 finished with value: 0.9423678575000001 and parameters: {'alpha': 933.0134569984682}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:43,676] Trial 51 finished with value: 0.9423665475 and parameters: {'alpha': 921.8312103598931}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:45,287] Trial 52 finished with value: 0.9422854239999999 and parameters: {'alpha': 463.43623083838753}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:46,870] Trial 53 finished with value: 0.9423669105 and parameters: {'alpha': 924.181955063294}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:48,451] Trial 54 finished with value: 0.9421484275 and parameters: {'alpha': 42.130756688731495}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:50,027] Trial 55 finished with value: 0.9422657114999999 and parameters: {'alpha': 387.03535275400736}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:51,610] Trial 56 finished with value: 0.942213844 and parameters: {'alpha': 215.51652018802494}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:53,183] Trial 57 finished with value: 0.9421835365 and parameters: {'alpha': 130.85949282404155}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:54,770] Trial 58 finished with value: 0.9423724454999999 and parameters: {'alpha': 973.7173244624288}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:56,350] Trial 59 finished with value: 0.9421306005 and parameters: {'alpha': 0.017891227764480173}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:57,954] Trial 60 finished with value: 0.9421520385000001 and parameters: {'alpha': 0.0003304923805695237}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:13:59,534] Trial 61 finished with value: 0.942298621 and parameters: {'alpha': 520.6397316460695}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:01,182] Trial 62 finished with value: 0.9423717884999998 and parameters: {'alpha': 968.4242702216951}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:02,750] Trial 63 finished with value: 0.9422943645 and parameters: {'alpha': 502.7332458716453}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:04,323] Trial 64 finished with value: 0.9421674080000001 and parameters: {'alpha': 88.74793662584686}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:05,906] Trial 65 finished with value: 0.9422295909999999 and parameters: {'alpha': 262.64002006301297}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:07,490] Trial 66 finished with value: 0.9423734620000002 and parameters: {'alpha': 983.2392180840802}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:09,063] Trial 67 finished with value: 0.9421490885000001 and parameters: {'alpha': 43.62043155573948}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:10,641] Trial 68 finished with value: 0.9422066515 and parameters: {'alpha': 194.61069333999802}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:12,231] Trial 69 finished with value: 0.9423183205000001 and parameters: {'alpha': 612.2961759581349}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:13,820] Trial 70 finished with value: 0.9421378414999999 and parameters: {'alpha': 17.97916525822784}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:15,406] Trial 71 finished with value: 0.9423579994999999 and parameters: {'alpha': 853.2244049494441}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:16,986] Trial 72 finished with value: 0.94225903 and parameters: {'alpha': 362.2006338880465}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:18,567] Trial 73 finished with value: 0.9421767735 and parameters: {'alpha': 113.14828571960706}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:20,141] Trial 74 finished with value: 0.9423184354999998 and parameters: {'alpha': 612.6694681291099}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:21,704] Trial 75 finished with value: 0.9423712710000001 and parameters: {'alpha': 963.076971790659}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:23,275] Trial 76 finished with value: 0.9422397565 and parameters: {'alpha': 295.7608340216727}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:24,884] Trial 77 finished with value: 0.9423735114999999 and parameters: {'alpha': 983.767578889745}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:26,462] Trial 78 finished with value: 0.942211665 and parameters: {'alpha': 209.10354413356734}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:28,032] Trial 79 finished with value: 0.9423060409999999 and parameters: {'alpha': 553.2062799720468}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:29,635] Trial 80 finished with value: 0.9422521400000001 and parameters: {'alpha': 337.49871180976976}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:31,241] Trial 81 finished with value: 0.9423745385 and parameters: {'alpha': 992.88465861548}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:32,827] Trial 82 finished with value: 0.942308695 and parameters: {'alpha': 565.1710380083267}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:34,384] Trial 83 finished with value: 0.9423718240000001 and parameters: {'alpha': 968.6750471245495}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:35,974] Trial 84 finished with value: 0.9421968845 and parameters: {'alpha': 166.7926803357085}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:37,551] Trial 85 finished with value: 0.9423744764999998 and parameters: {'alpha': 992.648599753633}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:39,134] Trial 86 finished with value: 0.9422570320000002 and parameters: {'alpha': 354.57820667820977}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:40,732] Trial 87 finished with value: 0.942173074 and parameters: {'alpha': 103.45158248094377}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:42,334] Trial 88 finished with value: 0.9421301795 and parameters: {'alpha': 0.49755565439560046}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:43,923] Trial 89 finished with value: 0.9423086009999999 and parameters: {'alpha': 564.8077728648051}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:45,493] Trial 90 finished with value: 0.9421542825 and parameters: {'alpha': 56.235095432819996}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:47,082] Trial 91 finished with value: 0.942330725 and parameters: {'alpha': 679.3741730095642}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:48,689] Trial 92 finished with value: 0.942230786 and parameters: {'alpha': 266.3169472778901}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:50,272] Trial 93 finished with value: 0.942367341 and parameters: {'alpha': 928.4140654038856}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:51,863] Trial 94 finished with value: 0.9422712055 and parameters: {'alpha': 406.96633179753593}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:53,462] Trial 95 finished with value: 0.9422141479999999 and parameters: {'alpha': 216.56907186571314}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:55,043] Trial 96 finished with value: 0.9423685589999998 and parameters: {'alpha': 938.4858572295807}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:56,617] Trial 97 finished with value: 0.942301464 and parameters: {'alpha': 533.2959427148985}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:58,195] Trial 98 finished with value: 0.942191328 and parameters: {'alpha': 152.07628658233784}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:14:59,777] Trial 99 finished with value: 0.942136684 and parameters: {'alpha': 0.001597899721332296}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:01,362] Trial 100 finished with value: 0.9422459029999999 and parameters: {'alpha': 316.1210917584464}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:02,952] Trial 101 finished with value: 0.9423259235 and parameters: {'alpha': 651.9351199338674}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:04,539] Trial 102 finished with value: 0.942372611 and parameters: {'alpha': 974.8617279532986}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:06,142] Trial 103 finished with value: 0.9422734335000001 and parameters: {'alpha': 414.6902667571161}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:07,731] Trial 104 finished with value: 0.9423262680000001 and parameters: {'alpha': 653.7225115808712}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:09,305] Trial 105 finished with value: 0.9423693679999999 and parameters: {'alpha': 944.905522214839}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:10,908] Trial 106 finished with value: 0.9422215735 and parameters: {'alpha': 238.59875490690686}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:12,476] Trial 107 finished with value: 0.9423266135 and parameters: {'alpha': 655.1584499985343}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:14,053] Trial 108 finished with value: 0.9422754720000001 and parameters: {'alpha': 422.8030602012689}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:15,635] Trial 109 finished with value: 0.9423734964999999 and parameters: {'alpha': 983.6941702719429}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:17,211] Trial 110 finished with value: 0.9421719675 and parameters: {'alpha': 100.47886197860596}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:18,835] Trial 111 finished with value: 0.9423738845 and parameters: {'alpha': 987.151309132459}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:20,404] Trial 112 finished with value: 0.9423300364999999 and parameters: {'alpha': 675.6351333754496}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:21,985] Trial 113 finished with value: 0.9422738815 and parameters: {'alpha': 416.7129153728131}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:23,578] Trial 114 finished with value: 0.9422020455 and parameters: {'alpha': 181.17702060964135}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:25,164] Trial 115 finished with value: 0.9422411195 and parameters: {'alpha': 300.4120488673662}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:26,755] Trial 116 finished with value: 0.9423262299999999 and parameters: {'alpha': 653.5645558846306}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:28,327] Trial 117 finished with value: 0.9423729135000001 and parameters: {'alpha': 978.7933961607238}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:29,912] Trial 118 finished with value: 0.942285336 and parameters: {'alpha': 462.89055160870726}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:31,493] Trial 119 finished with value: 0.9423705654999999 and parameters: {'alpha': 957.2623465436074}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:33,085] Trial 120 finished with value: 0.9422299890000001 and parameters: {'alpha': 263.9012855501849}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:34,655] Trial 121 finished with value: 0.942373981 and parameters: {'alpha': 987.8377205682499}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:36,230] Trial 122 finished with value: 0.942331111 and parameters: {'alpha': 681.9264042845189}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:37,865] Trial 123 finished with value: 0.9422820829999999 and parameters: {'alpha': 448.89130447256036}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:39,453] Trial 124 finished with value: 0.942370647 and parameters: {'alpha': 958.2565031416763}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:41,034] Trial 125 finished with value: 0.9422601110000001 and parameters: {'alpha': 365.8337863621987}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:42,657] Trial 126 finished with value: 0.942321393 and parameters: {'alpha': 627.0007261963442}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:44,227] Trial 127 finished with value: 0.9421302035 and parameters: {'alpha': 0.04283412080062719}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:45,817] Trial 128 finished with value: 0.94220092 and parameters: {'alpha': 178.16266357314223}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:47,386] Trial 129 finished with value: 0.9421695674999999 and parameters: {'alpha': 3.359707071537094e-05}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:49,000] Trial 130 finished with value: 0.9423743659999999 and parameters: {'alpha': 990.2511684739594}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:50,583] Trial 131 finished with value: 0.9422933194999998 and parameters: {'alpha': 497.83512504316377}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:52,153] Trial 132 finished with value: 0.9423363764999999 and parameters: {'alpha': 712.4987561734199}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:53,734] Trial 133 finished with value: 0.9422399225 and parameters: {'alpha': 296.27586489253054}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:55,336] Trial 134 finished with value: 0.9423713490000001 and parameters: {'alpha': 963.6140081908563}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:56,932] Trial 135 finished with value: 0.942372786 and parameters: {'alpha': 976.8924970661024}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:15:58,496] Trial 136 finished with value: 0.9422919160000001 and parameters: {'alpha': 492.0707327727407}. Best is trial 32 with value: 0.9423750314999999.\n",
      "[I 2024-07-09 21:16:00,088] Trial 137 finished with value: 0.942375052 and parameters: {'alpha': 997.2251478092703}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:01,675] Trial 138 finished with value: 0.942318352 and parameters: {'alpha': 612.4382170949443}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:03,260] Trial 139 finished with value: 0.942248209 and parameters: {'alpha': 323.91610008471656}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:04,835] Trial 140 finished with value: 0.9423371849999999 and parameters: {'alpha': 716.8685959339292}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:06,410] Trial 141 finished with value: 0.9423686444999999 and parameters: {'alpha': 939.3925806355101}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:07,988] Trial 142 finished with value: 0.942130691 and parameters: {'alpha': 1.7328615057950303}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:09,575] Trial 143 finished with value: 0.9423738259999999 and parameters: {'alpha': 986.7334022482025}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:11,146] Trial 144 finished with value: 0.942291745 and parameters: {'alpha': 490.8839850427313}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:12,726] Trial 145 finished with value: 0.9423291805 and parameters: {'alpha': 670.4278800651188}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:14,295] Trial 146 finished with value: 0.9422668184999999 and parameters: {'alpha': 390.92881086580707}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:15,890] Trial 147 finished with value: 0.9422264300000001 and parameters: {'alpha': 252.485064311068}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:17,468] Trial 148 finished with value: 0.9423743305000001 and parameters: {'alpha': 990.0885346464597}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:19,073] Trial 149 finished with value: 0.9422973545 and parameters: {'alpha': 515.2385056326807}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:20,655] Trial 150 finished with value: 0.9421331455 and parameters: {'alpha': 7.435504254868167}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:22,225] Trial 151 finished with value: 0.9423717855 and parameters: {'alpha': 968.4120563904955}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:23,812] Trial 152 finished with value: 0.9423323589999999 and parameters: {'alpha': 689.0883242003302}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:25,417] Trial 153 finished with value: 0.9422634015 and parameters: {'alpha': 379.1234535291754}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:26,985] Trial 154 finished with value: 0.9423293990000001 and parameters: {'alpha': 671.5478372062655}. Best is trial 137 with value: 0.942375052.\n",
      "[I 2024-07-09 21:16:28,572] Trial 155 finished with value: 0.9423752569999999 and parameters: {'alpha': 998.6457923317819}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:30,135] Trial 156 finished with value: 0.9422956635 and parameters: {'alpha': 508.21950399081453}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:31,727] Trial 157 finished with value: 0.9423715635000001 and parameters: {'alpha': 966.055731667124}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:33,292] Trial 158 finished with value: 0.9422505715 and parameters: {'alpha': 331.90146435142935}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:34,886] Trial 159 finished with value: 0.942325702 and parameters: {'alpha': 650.7171091153774}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:36,495] Trial 160 finished with value: 0.9422127175 and parameters: {'alpha': 212.0213861067668}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:38,096] Trial 161 finished with value: 0.9423723505 and parameters: {'alpha': 973.1786145308229}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:39,667] Trial 162 finished with value: 0.9423733160000001 and parameters: {'alpha': 982.2351121868295}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:41,245] Trial 163 finished with value: 0.9422944695 and parameters: {'alpha': 503.3256415514912}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:42,846] Trial 164 finished with value: 0.9423210175 and parameters: {'alpha': 625.3107748810367}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:44,418] Trial 165 finished with value: 0.9423749640000001 and parameters: {'alpha': 996.2676111717846}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:46,016] Trial 166 finished with value: 0.9422661089999999 and parameters: {'alpha': 388.32461484777355}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:47,614] Trial 167 finished with value: 0.9423742895 and parameters: {'alpha': 989.8521572521553}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:49,187] Trial 168 finished with value: 0.9423315565000001 and parameters: {'alpha': 684.1757287725837}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:50,764] Trial 169 finished with value: 0.9422898529999999 and parameters: {'alpha': 482.69344925298714}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:52,318] Trial 170 finished with value: 0.9423346240000001 and parameters: {'alpha': 702.5788047165465}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:53,918] Trial 171 finished with value: 0.9423712235 and parameters: {'alpha': 962.4516378928441}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:55,535] Trial 172 finished with value: 0.9423287260000001 and parameters: {'alpha': 667.8503951246503}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:57,148] Trial 173 finished with value: 0.9422806255000001 and parameters: {'alpha': 443.0252042687106}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:16:58,739] Trial 174 finished with value: 0.9423715749999999 and parameters: {'alpha': 966.1935376807446}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:00,316] Trial 175 finished with value: 0.9422438835000001 and parameters: {'alpha': 309.04999937702564}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:01,908] Trial 176 finished with value: 0.942340837 and parameters: {'alpha': 737.8632010452574}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:03,496] Trial 177 finished with value: 0.942313661 and parameters: {'alpha': 589.1791122305757}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:05,098] Trial 178 finished with value: 0.9423732565 and parameters: {'alpha': 981.7125128734702}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:06,706] Trial 179 finished with value: 0.942133814 and parameters: {'alpha': 0.00288737796783897}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:08,289] Trial 180 finished with value: 0.9422808420000001 and parameters: {'alpha': 443.82954204425056}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:09,877] Trial 181 finished with value: 0.9423729144999999 and parameters: {'alpha': 978.7778768538129}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:11,478] Trial 182 finished with value: 0.942340011 and parameters: {'alpha': 733.5254116474929}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:13,129] Trial 183 finished with value: 0.9421301484999999 and parameters: {'alpha': 0.361379890636209}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:14,729] Trial 184 finished with value: 0.9423728029999999 and parameters: {'alpha': 976.7656351057055}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:16,318] Trial 185 finished with value: 0.9423016315 and parameters: {'alpha': 534.3934562096571}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:17,907] Trial 186 finished with value: 0.94233505 and parameters: {'alpha': 704.9105825403021}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:19,499] Trial 187 finished with value: 0.9423740555 and parameters: {'alpha': 988.181444269596}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:21,109] Trial 188 finished with value: 0.942375195 and parameters: {'alpha': 998.1810989766749}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:22,707] Trial 189 finished with value: 0.942244427 and parameters: {'alpha': 310.9950032277961}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:24,301] Trial 190 finished with value: 0.9422874839999998 and parameters: {'alpha': 472.95781111196715}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:25,908] Trial 191 finished with value: 0.9423743305000001 and parameters: {'alpha': 990.1387414211395}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:27,489] Trial 192 finished with value: 0.9423330935 and parameters: {'alpha': 694.0593164321808}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:29,060] Trial 193 finished with value: 0.9423305230000001 and parameters: {'alpha': 678.3463145655979}. Best is trial 155 with value: 0.9423752569999999.\n",
      "[I 2024-07-09 21:17:30,680] Trial 194 finished with value: 0.9423753100000001 and parameters: {'alpha': 999.7257258166762}. Best is trial 194 with value: 0.9423753100000001.\n",
      "[I 2024-07-09 21:17:32,281] Trial 195 finished with value: 0.942294836 and parameters: {'alpha': 504.8178951364162}. Best is trial 194 with value: 0.9423753100000001.\n",
      "[I 2024-07-09 21:17:33,879] Trial 196 finished with value: 0.942332268 and parameters: {'alpha': 688.6818666135102}. Best is trial 194 with value: 0.9423753100000001.\n",
      "[I 2024-07-09 21:17:35,457] Trial 197 finished with value: 0.9421450995 and parameters: {'alpha': 0.0006130084996913844}. Best is trial 194 with value: 0.9423753100000001.\n",
      "[I 2024-07-09 21:17:37,038] Trial 198 finished with value: 0.9423742404999998 and parameters: {'alpha': 989.6166482835768}. Best is trial 194 with value: 0.9423753100000001.\n",
      "[I 2024-07-09 21:17:38,648] Trial 199 finished with value: 0.9422564695 and parameters: {'alpha': 352.37416297956423}. Best is trial 194 with value: 0.9423753100000001.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Ridge Classifier의 목적 함수\n",
    "def ridge_classifier_objective(trial):\n",
    "\n",
    "    alpha = trial.suggest_float('alpha', 1e-5, 1e3, log=True)\n",
    "    clf = RidgeClassifier(alpha=alpha)\n",
    "    \n",
    "    # 교차 검증을 통한 모델 평가 (AUC 스코어)\n",
    "    # 다중 클래스의 경우, 'ovr' 또는 'ovo' 스키마를 사용\n",
    "    # if len(set(y_train_resampled)) > 2:\n",
    "    #     scoring = 'roc_auc_ovr'\n",
    "    # else:\n",
    "    #     scoring = 'roc_auc'\n",
    "    \n",
    "    scores = cross_val_score(clf, X_train_resampled, y_train_resampled, cv=5, scoring='roc_auc')\n",
    "    return scores.mean()\n",
    "\n",
    "ridge_classifier_study = optuna.create_study(direction='maximize')\n",
    "ridge_classifier_study.optimize(ridge_classifier_objective, n_trials=200)\n",
    "ridge_classifier_best_params = ridge_classifier_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7661290322580645\n"
     ]
    }
   ],
   "source": [
    "# 최적의 하이퍼파라미터로 최종 모델 학습 및 평가\n",
    "model_ridge_classifier = RidgeClassifier(**ridge_classifier_best_params)\n",
    "model_ridge_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 검증 데이터로 예측\n",
    "y_pred = model_ridge_classifier.predict(X_val)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8757050765511684\n"
     ]
    }
   ],
   "source": [
    "# 최적의 하이퍼파라미터로 최종 모델 학습 및 평가\n",
    "model_ridge_classifier = RidgeClassifier(**ridge_classifier_best_params)\n",
    "model_ridge_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 검증 데이터로 예측\n",
    "y_pred = model_ridge_classifier.predict(X_val)\n",
    "y_pred_proba = model_ridge_classifier.decision_function(X_val)\n",
    "\n",
    "# 다중 클래스의 경우 ROC AUC 스코어 계산\n",
    "if len(set(y_val)) > 2:\n",
    "    y_val_bin = label_binarize(y_val, classes=list(set(y_val)))\n",
    "    auc_score = roc_auc_score(y_val_bin, y_pred_proba, multi_class='ovr')\n",
    "else:\n",
    "    auc_score = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logis=LogisticRegression(l1_ratio= 0.789370019111903, C= 0.07701480047825814, max_iter= 1000)\n",
    "model_logis.fit(X_train_resampled,y_train_resampled)\n",
    "y_pred = model_logis.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomforest feature selection  큰 효과가 없는 듯 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier()\n",
    "# model.fit(train_resampled, y_train_resampled)\n",
    "# y_pred = model.predict(test)\n",
    "\n",
    "# rn_features = []\n",
    "# importances = model.feature_importances_\n",
    "# feature_names = train.columns\n",
    "\n",
    "# # 피처 중요도를 기준으로 정렬하여 상위 피처 선택\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# # 중요도가 0.01 이상인 피처만 선택\n",
    "# # top_number = 40\n",
    "# top_num_indices = [idx for idx in indices if importances[idx] >= 0.005] #[:top_number]\n",
    "# top_features = feature_names[top_num_indices]\n",
    "\n",
    "# for i, feature in enumerate(top_features):\n",
    "#     print(f\"{i+1}. {feature} (중요도: {importances[top_num_indices[i]]})\")\n",
    "#     rn_features.append(feature)\n",
    "\n",
    "# rn_train_resampled = train_resampled[rn_features]\n",
    "# rn_test = test[rn_features]\n",
    "\n",
    "# model_logis=LogisticRegression(**logreg_best_params)\n",
    "# model_logis.fit(rn_train_resampled,y_train_resampled)\n",
    "# y_pred = model_logis.predict(rn_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상관계수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X_train.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "corr_number = 0.9\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > corr_number)]\n",
    "to_drop\n",
    "# # 특징 제거\n",
    "corr_X_train_resampled = X_train_resampled.drop(columns=to_drop)  \n",
    "corr_X_val = X_val.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_objective(trial):\n",
    "    \n",
    "    r = trial.suggest_float('l1_ratio', 0, 1, log=False)\n",
    "    c = trial.suggest_float('C', 1e-4, 1e2, log=True)\n",
    "     \n",
    "    clf =  LogisticRegression(max_iter=5000, solver='saga', penalty='elasticnet', l1_ratio=r, C=c)\n",
    "    scores = cross_val_score(clf, corr_X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return scores.mean()\n",
    "    \n",
    "logreg_study = optuna.create_study(direction='maximize')\n",
    "logreg_study.optimize(logreg_objective, n_trials=5)\n",
    "\n",
    "logreg_best_params = logreg_study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logis=LogisticRegression(**logreg_best_params)\n",
    "model_logis.fit(corr_X_train_resampled,y_train_resampled)\n",
    "y_pred = model_logis.predict(corr_X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 규제(Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha 값 후보군 설정\n",
    "alpha_values = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 1, 10, 100]\n",
    "param_grid = {'alpha': alpha_values}\n",
    "\n",
    "# Lasso 모델과 GridSearchCV 설정\n",
    "lasso = Lasso()\n",
    "grid_search = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# 최적의 alpha 값 찾기\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=best_alpha)  # 위에서 나온 alpha 값으로 조정한 거임\n",
    "lasso.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 가중치가 0이 아닌 특징 선택\n",
    "selected_features = X_train_resampled.columns[lasso.coef_ != 0]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_X_train_resampled = X_train_resampled[selected_features]\n",
    "L1_X_val = X_val[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_objective(trial):\n",
    "    \n",
    "    r = trial.suggest_float('l1_ratio', 0, 1, log=False)\n",
    "    c = trial.suggest_float('C', 1e-4, 1e2, log=True)\n",
    "     \n",
    "    clf =  LogisticRegression(max_iter=5000, solver='saga', penalty='elasticnet', l1_ratio=r, C=c)\n",
    "    scores = cross_val_score(clf, L1_X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return scores.mean()\n",
    "    \n",
    "logreg_study = optuna.create_study(direction='maximize')\n",
    "logreg_study.optimize(logreg_objective, n_trials=5)\n",
    "\n",
    "logreg_best_params = logreg_study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2013~2023  \n",
    "### 많긴 하지만 정확도 떨어질 것으로 예상됨\n",
    "\n",
    "# 2020~2023\n",
    "### 데이터 수가 급격히 줄어듬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
