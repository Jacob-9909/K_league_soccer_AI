{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from matplotlib import font_manager, rc\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "# from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, PowerTransformer, OrdinalEncoder,\n",
    "    OneHotEncoder, FunctionTransformer, PolynomialFeatures, LabelEncoder, MinMaxScaler\n",
    ")\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, RFE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression, LinearRegression, Ridge, Lasso,\n",
    "    SGDRegressor, ElasticNet\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, cross_validate,\n",
    "    GridSearchCV, KFold, cross_val_predict\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, mean_squared_error, make_scorer, accuracy_score, log_loss\n",
    ")\n",
    "from sklearn import set_config, datasets\n",
    "from catboost import (\n",
    "    CatBoostRegressor, CatBoostClassifier,\n",
    ")\n",
    "# import category_encoders as ce\n",
    "# from sklearn.pipeline import (\n",
    "#     Pipeline, FeatureUnion, make_pipeline\n",
    "# )\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, StackingClassifier, StackingRegressor,\n",
    "    GradientBoostingRegressor, VotingClassifier, VotingRegressor,\n",
    "    HistGradientBoostingRegressor, GradientBoostingClassifier,\n",
    "    BaggingClassifier, AdaBoostClassifier, RandomForestRegressor,ExtraTreesRegressor\n",
    ")\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# from tpot import TPOTClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import math\n",
    "import optuna\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('adidas_match_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('matches_adidas_ver2_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 'home_team_result' 무 -> 승,패로 업데이트, 추가적인 기준 필요함\n",
    "# df.loc[(df['home_team_result'] == '무') & (df['home_team_shots_on_target'] >= df['away_team_shots_on_target']), 'home_team_result'] = '승'\n",
    "# df.loc[(df['home_team_result'] == '무') & (df['home_team_shots_on_target'] < df['away_team_shots_on_target']), 'home_team_result'] = '패'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature fun\n",
    "def team_encoding(train):\n",
    "    train['home_win'] = train['home_team_result'].apply(lambda x: 1 if x=='승' else 0) # home_win 열 추가, 승리인 경우 1, 아닌 경우 0\n",
    "    dic = {}\n",
    "    # 각 홈팀별 이긴 경기 수를 딕셔너리에 저장\n",
    "    for team in train['home_team_name'].unique():\n",
    "        value = train[train['home_team_name'] == team]['home_win'].sum() \n",
    "        #home_team_name  열에서 고유한 팀 이름을 가져와 각 팀이 홈에서 이긴 경기 수 계산, 이 값을 dic에 저장\n",
    "        dic[team] = value\n",
    "\n",
    "    label_dic={}\n",
    "    # 승리 횧수를 기준으로 오름차순 정렬, 각 팀에 대해 라벨 부여, 승리 횟수가 적은 팀부터 0,1,2 의 라벨을 부여\n",
    "    for idx, (team, _) in enumerate(sorted(dic.items(), key= lambda x: x[1])):\n",
    "        label_dic[team] = idx\n",
    "    \n",
    "    return label_dic\n",
    "\n",
    "\n",
    "''' 홈팀 득점 이동평균 계산 함수 '''\n",
    "\n",
    "def homeGoal_day_mean(train, test, day):\n",
    "    train[f'home_Goal_{day}_mean'] = -1  # 초기값 -1로 설정\n",
    "    test[f'home_Goal_{day}_mean'] = -1\n",
    "    \n",
    "    teams = train['home_team_name'].unique()\n",
    "    for team in tqdm(teams): # train에서 고유 팀 이름을 가져오고 이를 시각적으로 표시해줌 : tqdm\n",
    "        team_df = train[train['home_team_name'] == team]\n",
    "        # 롤링 윈도우 크기 설정\n",
    "        ch_day = len(team_df) if len(team_df) < day else day # 팀의 경기 수가 주어진 day 보다 적으면, 경기 수 만큼의 윈도우 크기 사용\n",
    "        idx = team_df['home_team_goal_count'].rolling(ch_day).mean().index.values # 롤링 윈도우 평균 계산\n",
    "        val = team_df['home_team_goal_count'].rolling(ch_day).mean().values\n",
    "        train[f'home_Goal_{day}_mean'].loc[idx] = val\n",
    "        test_idx = test[test['home_team_name'] == team].index\n",
    "        test[f'home_Goal_{day}_mean'].loc[test_idx] = val[-1]\n",
    "    # 결측값 처리\n",
    "    train[f'home_Goal_{day}_mean'] = train[f'home_Goal_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "''' 원정팀 득점 이동평균 계산 함수 '''\n",
    "\n",
    "def awayGoal_day_mean(train, test, day):\n",
    "    # 초기값 설정\n",
    "    train[f'away_Goal_{day}_mean'] = -1\n",
    "    test[f'away_Goal_{day}_mean'] = -1\n",
    "    \n",
    "    teams = train['away_team_name'].unique()\n",
    "    for team in tqdm(teams):\n",
    "        team_df = train[train['away_team_name'] == team]\n",
    "        # 롤링 윈도우 크기 설정\n",
    "        ch_day = len(team_df) if len(team_df) < day else day\n",
    "        idx = team_df['away_team_goal_count'].rolling(ch_day).mean().index.values\n",
    "        val = team_df['away_team_goal_count'].rolling(ch_day).mean().values\n",
    "        train[f'away_Goal_{day}_mean'].loc[idx] = val\n",
    "        test_idx = test[test['away_team_name'] == team].index\n",
    "        test[f'away_Goal_{day}_mean'].loc[test_idx] = val[-1]\n",
    "    # 결측값 처리\n",
    "    train[f'away_Goal_{day}_mean'] = train[f'away_Goal_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "# '''홈팀 승리율 평균 계산 함수'''\n",
    "\n",
    "# def homeWin_day_mean(train, test, day):\n",
    "#     train[f'home_winRate_{day}_mean'] = -1\n",
    "#     test[f'home_winRate_{day}_mean'] = -1\n",
    "#     train['win'] = train['home_team_result'].apply(lambda x: 1 if x == '승' else 0)\n",
    "\n",
    "#     teams = train['home_team_name'].unique()\n",
    "#     for team in tqdm(teams):\n",
    "#         team_df = train[train['home_team_name'] == team]\n",
    "#         ch_day = len(team_df) if len(team_df) < day else day\n",
    "#         idx = team_df['win'].rolling(ch_day).mean().index.values\n",
    "#         val = team_df['win'].rolling(ch_day).mean().values\n",
    "#         train[f'home_winRate_{day}_mean'].loc[idx] = val\n",
    "#         test_idx = test[test['home_team_name'] == team].index\n",
    "#         test[f'home_winRate_{day}_mean'].loc[test_idx] = val[-1]\n",
    "\n",
    "#     train.drop(columns=['win'], inplace=True)\n",
    "\n",
    "#     train[f'home_winRate_{day}_mean'] = train[f'home_winRate_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "# '''원정팀 승리율 평균 계산 함수'''\n",
    "\n",
    "# def awayWin_day_mean(train, test, day):\n",
    "\n",
    "#     train[f'away_winRate_{day}_mean'] = -1\n",
    "#     test[f'away_winRate_{day}_mean'] = -1\n",
    "#     train['win'] = train['home_team_result'].apply(lambda x: 1 if x == '패' else 0)\n",
    "    \n",
    "#     teams = train['away_team_name'].unique()\n",
    "#     for team in tqdm(teams):\n",
    "#         team_df = train[train['away_team_name'] == team]\n",
    "\n",
    "#         ch_day = len(team_df) if len(team_df) < day else day\n",
    "#         idx = team_df['win'].rolling(ch_day).mean().index.values\n",
    "#         val = team_df['win'].rolling(ch_day).mean().values\n",
    "#         train[f'away_winRate_{day}_mean'].loc[idx] = val\n",
    "#         test_idx = test[test['away_team_name'] == team].index\n",
    "#         test[f'away_winRate_{day}_mean'].loc[test_idx] = val[-1]\n",
    "\n",
    "#     train.drop(columns=['win'], inplace=True)\n",
    "\n",
    "#     train[f'away_winRate_{day}_mean'] = train[f'away_winRate_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "'''홈팀 평균 계산 함수'''\n",
    "\n",
    "def home_day_mean(train, test, columns, day):\n",
    "    for column in tqdm(columns):\n",
    "        teams = train['home_team_name'].values\n",
    "        train[f'home_{column}_{day}_mean'] = -1\n",
    "        test[f'home_{column}_{day}_mean'] = -1\n",
    "\n",
    "        for team in tqdm(teams):\n",
    "            team_df = train[train['home_team_name'] == team]\n",
    "            idx = team_df[column].rolling(day).mean().index.values\n",
    "            val = team_df[column].rolling(day).mean().values\n",
    "            train[f'home_{column}_{day}_mean'].loc[idx] = val\n",
    "            test_idx = test[test['home_team_name'] == team].index\n",
    "            test[f'home_{column}_{day}_mean'].loc[test_idx] = val[-1]\n",
    "\n",
    "        train[f'home_{column}_{day}_mean'] = train[f'home_{column}_{day}_mean'].fillna(0)\n",
    "        test[f'home_{column}_{day}_mean'] = test[f'home_{column}_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "'''원정팀 평균 계산 함수'''\n",
    "\n",
    "def away_day_mean(train, test, columns, day):\n",
    "    for column in tqdm(columns):\n",
    "        teams = train['away_team_name'].values\n",
    "        train[f'away_{column}_{day}_mean'] = -1\n",
    "        test[f'away_{column}_{day}_mean'] = -1\n",
    "\n",
    "        for team in tqdm(teams):\n",
    "            team_df = train[train['away_team_name'] == team]\n",
    "            idx = team_df[column].rolling(day).mean().index.values\n",
    "            val = team_df[column].rolling(day).mean().values\n",
    "            train[f'away_{column}_{day}_mean'].loc[idx] = val\n",
    "            test_idx = test[test['away_team_name'] == team].index\n",
    "            test[f'away_{column}_{day}_mean'].loc[test_idx] = val[-1]\n",
    "\n",
    "        train[f'away_{column}_{day}_mean'] = train[f'away_{column}_{day}_mean'].fillna(0)\n",
    "        test[f'away_{column}_{day}_mean'] = test[f'away_{column}_{day}_mean'].fillna(0)\n",
    "\n",
    "\n",
    "'''전처리 함수'''\n",
    "\n",
    "def preprocessing(train, test):\n",
    "    # 년과 월일로 나누기\n",
    "    train['date_GMT'] = train['date_GMT'].dt.strftime('%Y%m%d')\n",
    "    train['year'] = train['date_GMT'].apply(lambda x : int(x[0:4]))\n",
    "    train['date'] = train['date_GMT'].apply(lambda x : int(x[4:10]))\n",
    "    \n",
    "    test['date_GMT'] = test['date_GMT'].dt.strftime('%Y%m%d')\n",
    "    test['year'] = test['date_GMT'].apply(lambda x : int(x[0:4]))\n",
    "    test['date'] = test['date_GMT'].apply(lambda x : int(x[4:10]))\n",
    "\n",
    "    # train.drop(columns=['date_GMT'], inplace=True)\n",
    "    # test.drop(columns=['date_GMT'], inplace=True)\n",
    "\n",
    "    # # 팀 인코딩 적용   # 위에서 적용 했음\n",
    "    # label_dic = dic\n",
    "    # train['home_team_name'] = train['home_team_name'].apply(lambda x: label_dic[x])\n",
    "    # train['away_team_name'] = train['away_team_name'].apply(lambda x: label_dic[x])\n",
    "    # test['home_team_name'] = test['home_team_name'].apply(lambda x: label_dic[x])\n",
    "    # test['away_team_name'] = test['away_team_name'].apply(lambda x: label_dic[x])\n",
    "\n",
    "    # # 5일간 홈팀 승리 비율 계산    ### 이거 쓰레기인듯\n",
    "    # homeWin_day_mean(train, test, 5)\n",
    "    # # 5일간 원정팀 승리 비율 계산\n",
    "    # awayWin_day_mean(train, test, 5)\n",
    "\n",
    "    # 5일간 홈팀 평균 득점 계산\n",
    "    homeGoal_day_mean(train, test, 5)\n",
    "\n",
    "    # 5일간 원정팀 평균 득점 계산\n",
    "    awayGoal_day_mean(train, test, 5)\n",
    "\n",
    "    # 불필요한 컬럼 제거\n",
    "    # train = train.drop(columns=['index','home_team_goal_count','away_team_goal_count','game_points'])\n",
    "    # test = test.drop(columns=['index','home_team_goal_count','away_team_goal_count','home_team_result','game_points'])\n",
    "\n",
    "    train = train.drop(columns=['index','home_team_goal_count','away_team_goal_count'])\n",
    "    test = test.drop(columns=['index','home_team_goal_count','away_team_goal_count','home_team_result'])\n",
    "\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_label = {\n",
    "    '승' : 0,\n",
    "    '패' : 1, \n",
    "    '무' : 2,} \n",
    "    \n",
    "X = df.drop(columns=['away_team_possession'])\n",
    "X['home_team_result'] = X['home_team_result'].map(result_label)\n",
    "y = X['home_team_result']\n",
    "# split_index = 1796 # 2021년도까지의 index\n",
    "# train = X.iloc[:split_index]\n",
    "# test = X.iloc[split_index:]\n",
    "# y_train = y.iloc[:split_index]\n",
    "# y_test = y.iloc[split_index:]\n",
    "\n",
    "# team_name 인코딩\n",
    "cat = ['home_team_name','away_team_name']\n",
    "le = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=int)\n",
    "X[cat] = le.fit_transform(X[cat])\n",
    "\n",
    "# # 승무패 인코딩\n",
    "# lec = LabelEncoder()\n",
    "# lec.fit(X['home_team_result'])\n",
    "# y = lec.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 635.73it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 666.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(547, 132) (137, 131)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train,Test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) ## 이 방법에 의문을 품는 바\n",
    "\n",
    "X_train, X_val= preprocessing(X_train, X_val)\n",
    "X_val_idx = X_val.index.values\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 홈팀과 원정팀의 공격 효율성을 계산한 피쳐 생성\n",
    "X_train['home_attack_efficiency'] = X_train['home_Goal_5_mean'] * X_train['home_team_shots_on_target']\n",
    "X_train['away_attack_efficiency'] = X_train['away_Goal_5_mean'] * X_train['away_team_shots_on_target']\n",
    "\n",
    "# 홈팀과 원정팀의 공격 효율성 차이를 나타내는 피쳐 생성\n",
    "X_train['attack_efficiency_difference'] = X_train['home_attack_efficiency'] - X_train['away_attack_efficiency']\n",
    "\n",
    "# 홈팀과 원정팀의 전반 골 수 차이를 나타내는 피쳐 생성\n",
    "X_train['goal_count_diff'] = X_train['home_team_goal_count_half_time'] - X_train['away_team_goal_count_half_time']\n",
    "\n",
    "# 최근 5경기 평균 득점의 표준 편차를 나타내는 피쳐 생성\n",
    "X_train['home_Goal_5_std'] = X_train['home_Goal_5_mean'].rolling(window=5).std()\n",
    "X_train['away_Goal_5_std'] = X_train['away_Goal_5_mean'].rolling(window=5).std()\n",
    "\n",
    "# 결측값을 0으로 대체\n",
    "X_train = X_train.fillna(0)\n",
    "\n",
    "# 테스트 데이터에도 동일한 피쳐 생성\n",
    "X_val['home_attack_efficiency'] = X_val['home_Goal_5_mean'] * X_val['home_team_shots_on_target']\n",
    "X_val['away_attack_efficiency'] = X_val['away_Goal_5_mean'] * X_val['away_team_shots_on_target']\n",
    "X_val['attack_efficiency_difference'] = X_val['home_attack_efficiency'] - X_val['away_attack_efficiency']\n",
    "X_val['goal_count_diff'] = X_val['home_team_goal_count_half_time'] - X_val['away_team_goal_count_half_time']\n",
    "X_val['home_Goal_5_std'] = X_val['home_Goal_5_mean'].rolling(window=5).std()\n",
    "X_val['away_Goal_5_std'] = X_val['away_Goal_5_mean'].rolling(window=5).std()\n",
    "X_val = X_val.fillna(0)\n",
    "\n",
    "# 학습 데이터에서 목표 변수 'home_team_result' 컬럼 제거\n",
    "X_train.drop(columns = ['home_team_result'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler\n",
    "cat = ['home_team_name','away_team_name']\n",
    "\n",
    "num_features = list(set(X_train.columns) - set(cat))\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "X_train[num_features] = scaler.fit_transform(X_train[num_features])\n",
    "X_val[num_features] = scaler.transform(X_val[num_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled class distribution: Counter({0: 10000, 2: 10000, 1: 10000})\n",
      "Original training set shape: (547, 137)\n",
      "Resampled training set shape: (30000, 137)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# SMOTE 객체 생성 (각 클래스의 샘플 수를 1000개로 설정)\n",
    "smote = SMOTE(sampling_strategy={0: 10000, 1: 10000, 2 : 10000}, random_state=42)\n",
    "\n",
    "# SMOTE-Tomek 객체 생성\n",
    "smote_tomek = SMOTETomek(smote=smote, random_state=42)\n",
    "\n",
    "# 오버샘플링 및 언더샘플링 적용\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# 각 클래스 비율 확인\n",
    "print(f\"Resampled class distribution: {Counter(y_train_resampled)}\")\n",
    "print(f\"Original training set shape: {X_train.shape}\")\n",
    "print(f\"Resampled training set shape: {X_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled['home_team_result'] = y_train_resampled\n",
    "X_val['home_team_result'] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'home_team_result' 무 -> 승,패로 업데이트, 추가적인 기준 필요함\n",
    "X_train_resampled.loc[(X_train_resampled['home_team_result'] == 2) & (X_train_resampled['home_team_shots_on_target'] >= X_train_resampled['away_team_shots_on_target']), 'home_team_result'] = 0\n",
    "X_train_resampled.loc[(X_train_resampled['home_team_result'] == 2) & (X_train_resampled['home_team_shots_on_target'] < X_train_resampled['away_team_shots_on_target']), 'home_team_result'] = 1\n",
    "# 'home_team_result' 무 -> 승,패로 업데이트, 추가적인 기준 필요함\n",
    "X_val.loc[(X_val['home_team_result'] == 2) & (X_val['home_team_shots_on_target'] >= X_val['away_team_shots_on_target']), 'home_team_result'] = 0\n",
    "X_val.loc[(X_val['home_team_result'] == 2) & (X_val['home_team_shots_on_target'] < X_val['away_team_shots_on_target']), 'home_team_result'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 값 배정\n",
    "y_train_resampled = X_train_resampled['home_team_result']\n",
    "y_val = X_val['home_team_result']\n",
    "# target 값 다시 삭제\n",
    "X_train_resampled.drop(columns='home_team_result', inplace = True)\n",
    "X_val.drop(columns='home_team_result', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pycaret.classification import *\n",
    "\n",
    "# setup_clf = setup(data = X_train, target = y_train, session_id = 42)\n",
    "# model = compare_models(sort = 'Accuracy', fold = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_tune = tune_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model(best_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8248175182481752\n",
      "Filtered SHAP Importances:\n",
      "                       column_name  shap_importance\n",
      "7       home_team_shots_on_target         1.767909\n",
      "133  attack_efficiency_difference         1.758972\n",
      "8       away_team_shots_on_target         1.490812\n",
      "134               goal_count_diff         1.302669\n",
      "22              adidas_point_home         1.129522\n",
      "23              adidas_point_away         1.096887\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import shap\n",
    "SHAP_THRESHOLD = 1\n",
    "\n",
    "# feature_names dimension 조정\n",
    "X_train_col = X_train.columns\n",
    "feature_names = X_train_col.to_numpy()\n",
    "\n",
    "# 모델 학습\n",
    "model = xgb.XGBClassifier().fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 모델 예측 및 평가\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "# SHAP 값 계산\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_val)\n",
    "\n",
    "# SHAP 값 요약\n",
    "if isinstance(shap_values, list):  # shap_values가 리스트일 경우 (XGBoost >= 1.0.0)\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "importance_df = pd.DataFrame({'column_name': feature_names, 'shap_importance': shap_sum})\n",
    "importance_df = importance_df.sort_values('shap_importance', ascending=False)\n",
    "\n",
    "# 중요도 임계값 적용 (선택 사항)\n",
    "importance_df_filtered = importance_df[importance_df['shap_importance'] > SHAP_THRESHOLD]\n",
    "print(\"Filtered SHAP Importances:\\n\", importance_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지정된(SHAP_THRESHOLD) Shap feature 중요도 이상인 것만 선택\n",
    "features_selected = importance_df.query('shap_importance > @SHAP_THRESHOLD').column_name.tolist()\n",
    "shap_xgb_X_train_resampled = X_train_resampled[features_selected]\n",
    "shap_xgb_X_val = X_val[features_selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LightGBM 모델 학습\n",
    "# model = lgb.LGBMClassifier().fit(X_train_resampled, y_train_resampled)\n",
    "# SHAP_THRESHOLD = 0.1\n",
    "\n",
    "# # 모델 예측 및 평가\n",
    "# y_pred = model.predict(X_val)\n",
    "# print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "# # SHAP 값 계산\n",
    "# explainer = shap.TreeExplainer(model)\n",
    "# shap_values = explainer.shap_values(X_val)\n",
    "\n",
    "# shap_sum = np.abs(shap_values).mean(axis=1)[1,:]\n",
    "# importance_df = pd.DataFrame([X_val.columns.tolist(), shap_sum.tolist()]).T\n",
    "# importance_df.columns = ['column_name', 'shap_importance']\n",
    "# importance_df = importance_df.sort_values('shap_importance', ascending=False);\n",
    "# importance_df\n",
    "\n",
    "# # SHAP 값 데이터프레임 생성 (각 피쳐별 SHAP 값)\n",
    "# shap_values_df = pd.DataFrame(shap_values[1], columns=X_val.columns)\n",
    "# shap_values_df\n",
    "\n",
    "# # SHAP 값의 평균 절대값 계산\n",
    "# shap_abs_mean = pd.DataFrame(shap_values[1], columns=X_val.columns).abs().mean().sort_values(ascending=False)\n",
    "# # 중요도 임계값 적용 (선택 사항)\n",
    "# importance_df_filtered = importance_df[importance_df['shap_importance'] > SHAP_THRESHOLD]\n",
    "# print(\"Filtered SHAP Importances:\\n\", importance_df_filtered)\n",
    "# # # SHAP 값 평균 절대값 시각화\n",
    "# # plt.figure(figsize=(10, 8))\n",
    "# # shap_abs_mean.plot(kind='barh')\n",
    "# # plt.title(\"Mean Absolute SHAP Values for Features\")\n",
    "# # plt.xlabel(\"Mean Absolute SHAP Value\")\n",
    "# # plt.ylabel(\"Features\")\n",
    "# # plt.gca().invert_yaxis()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 지정된(SHAP_THRESHOLD) Shap feature 중요도 이상인 것만 선택\n",
    "# features_selected = importance_df.query('shap_importance > @SHAP_THRESHOLD').column_name.tolist()\n",
    "# shap_lgbm_X_train_resampled = X_train_resampled[features_selected]\n",
    "# shap_lgbm_X_val = X_val[features_selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oputna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# def lgbm_objective(trial):\n",
    "#     # 하이퍼파라미터 범위 설정\n",
    "#     max_depth = trial.suggest_int('max_depth', 3, 7)\n",
    "#     learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "#     n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "#     subsample = trial.suggest_float('subsample', 0.5, 0.9)\n",
    "#     colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 0.9)\n",
    "#     min_child_weight = trial.suggest_int('min_child_weight', 4, 10)\n",
    "#     reg_alpha = trial.suggest_float('reg_alpha', 0, 10)  # 추가: L2 정규화\n",
    "#     reg_lambda = trial.suggest_float('reg_lambda', 0, 10)  # 추가: L1 정규화\n",
    "    \n",
    "#     # LGBMClassifier 모델 정의\n",
    "#     clf = LGBMClassifier(\n",
    "#         max_depth=max_depth,\n",
    "#         learning_rate=learning_rate,\n",
    "#         n_estimators=n_estimators,\n",
    "#         subsample=subsample,\n",
    "#         colsample_bytree=colsample_bytree,\n",
    "#         min_child_weight=min_child_weight,\n",
    "#         reg_alpha=reg_alpha,  # 추가\n",
    "#         reg_lambda=reg_lambda,  # 추가\n",
    "#         force_col_wise=True\n",
    "#     )\n",
    "    \n",
    "#     # 교차 검증 점수 계산\n",
    "#     scores = cross_val_score(clf, shap_lgbm_X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "    \n",
    "#     return scores.mean()\n",
    "\n",
    "# # Optuna 스터디 생성 및 최적화\n",
    "# lgbm_study = optuna.create_study(direction='maximize')\n",
    "# lgbm_study.optimize(lgbm_objective, n_trials=5)\n",
    "\n",
    "# # 최적 하이퍼파라미터 출력\n",
    "# lgbm_best_params = lgbm_study.best_params\n",
    "# print('최적화된 하이퍼파라미터:', lgbm_best_params)\n",
    "# print('최적 교차 검증 점수:', lgbm_study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LGBMClassifier 모델 정의 및 학습\n",
    "# model_logis = LGBMClassifier(**lgbm_best_params)\n",
    "# model_logis.fit(shap_lgbm_X_train_resampled, y_train_resampled)\n",
    "\n",
    "# # 예측 수행\n",
    "# y_pred = model_logis.predict(shap_lgbm_X_val)\n",
    "\n",
    "# # 정확도 계산\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(\"Accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-24 22:10:56,139] A new study created in memory with name: no-name-fb3f2f05-0da4-4f19-b710-2a76da41b036\n",
      "[I 2024-07-24 22:10:56,585] Trial 0 finished with value: 0.7318666666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.0011990023280616683, 'n_estimators': 64, 'subsample': 0.8127560461534247, 'colsample_bytree': 0.7250681686321334, 'min_child_weight': 8, 'gamma': 4.066803549381865}. Best is trial 0 with value: 0.7318666666666667.\n",
      "[I 2024-07-24 22:10:57,619] Trial 1 finished with value: 0.8702666666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.006823226434420083, 'n_estimators': 180, 'subsample': 0.5971580974541202, 'colsample_bytree': 0.6993211610299901, 'min_child_weight': 8, 'gamma': 2.2138689395316895}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:10:58,578] Trial 2 finished with value: 0.8402666666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.001046685144821813, 'n_estimators': 159, 'subsample': 0.5586451188134314, 'colsample_bytree': 0.7194532905306759, 'min_child_weight': 7, 'gamma': 3.4104253175294303}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:10:58,945] Trial 3 finished with value: 0.8379 and parameters: {'max_depth': 5, 'learning_rate': 0.009508919933080518, 'n_estimators': 76, 'subsample': 0.781045639432192, 'colsample_bytree': 0.7156734576196849, 'min_child_weight': 8, 'gamma': 4.653957521457486}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:10:59,422] Trial 4 finished with value: 0.7026 and parameters: {'max_depth': 4, 'learning_rate': 0.0007359349318523453, 'n_estimators': 128, 'subsample': 0.7464352862880074, 'colsample_bytree': 0.6033556650110602, 'min_child_weight': 10, 'gamma': 3.306763982960692}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:10:59,934] Trial 5 finished with value: 0.7853666666666667 and parameters: {'max_depth': 3, 'learning_rate': 0.0031739625564843007, 'n_estimators': 167, 'subsample': 0.5365244809450996, 'colsample_bytree': 0.8394934979611754, 'min_child_weight': 6, 'gamma': 3.9426351653719287}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:00,513] Trial 6 finished with value: 0.8473333333333333 and parameters: {'max_depth': 6, 'learning_rate': 0.002791886320604061, 'n_estimators': 113, 'subsample': 0.7259879751596326, 'colsample_bytree': 0.7650817645074968, 'min_child_weight': 6, 'gamma': 2.71863127726167}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:00,987] Trial 7 finished with value: 0.5766 and parameters: {'max_depth': 4, 'learning_rate': 0.0005260699444616388, 'n_estimators': 123, 'subsample': 0.8241274034386552, 'colsample_bytree': 0.5923591088530483, 'min_child_weight': 7, 'gamma': 4.161950537246117}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:01,276] Trial 8 finished with value: 0.6856333333333334 and parameters: {'max_depth': 4, 'learning_rate': 0.0013350219893700317, 'n_estimators': 67, 'subsample': 0.5902866613355812, 'colsample_bytree': 0.558909755779278, 'min_child_weight': 4, 'gamma': 0.9765739845902632}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:01,560] Trial 9 finished with value: 0.5281666666666667 and parameters: {'max_depth': 4, 'learning_rate': 0.0001079365430470523, 'n_estimators': 63, 'subsample': 0.6968069939737193, 'colsample_bytree': 0.708047984353761, 'min_child_weight': 5, 'gamma': 1.3931935510735967}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:02,601] Trial 10 finished with value: 0.859 and parameters: {'max_depth': 6, 'learning_rate': 0.008578792317358948, 'n_estimators': 199, 'subsample': 0.6454023226989117, 'colsample_bytree': 0.8518542226086612, 'min_child_weight': 10, 'gamma': 0.023587771217497444}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:03,629] Trial 11 finished with value: 0.8592000000000001 and parameters: {'max_depth': 6, 'learning_rate': 0.009303721642295818, 'n_estimators': 194, 'subsample': 0.6375321868817818, 'colsample_bytree': 0.8977162585848252, 'min_child_weight': 10, 'gamma': 0.06621532808525504}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:04,693] Trial 12 finished with value: 0.8461666666666666 and parameters: {'max_depth': 6, 'learning_rate': 0.0036317271251817104, 'n_estimators': 198, 'subsample': 0.6317855324920171, 'colsample_bytree': 0.8934028196712096, 'min_child_weight': 9, 'gamma': 1.7925545831711143}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:05,655] Trial 13 finished with value: 0.8587333333333333 and parameters: {'max_depth': 7, 'learning_rate': 0.005494104152782773, 'n_estimators': 169, 'subsample': 0.6258399656829814, 'colsample_bytree': 0.6422936135002498, 'min_child_weight': 9, 'gamma': 0.09524408252774119}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:06,549] Trial 14 finished with value: 0.6179333333333333 and parameters: {'max_depth': 6, 'learning_rate': 0.0003427248701508372, 'n_estimators': 181, 'subsample': 0.5146922753953418, 'colsample_bytree': 0.7729547716235401, 'min_child_weight': 9, 'gamma': 2.128424882465769}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:07,171] Trial 15 finished with value: 0.8326333333333332 and parameters: {'max_depth': 5, 'learning_rate': 0.0022266657405399953, 'n_estimators': 148, 'subsample': 0.8824998770015976, 'colsample_bytree': 0.6494565302507442, 'min_child_weight': 8, 'gamma': 0.8720376251123902}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:08,238] Trial 16 finished with value: 0.8596999999999999 and parameters: {'max_depth': 7, 'learning_rate': 0.005454563068742694, 'n_estimators': 181, 'subsample': 0.6783705735073949, 'colsample_bytree': 0.5025303545873492, 'min_child_weight': 10, 'gamma': 0.5811758142860342}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:09,055] Trial 17 finished with value: 0.8561 and parameters: {'max_depth': 7, 'learning_rate': 0.005212351402724058, 'n_estimators': 141, 'subsample': 0.6820036642280366, 'colsample_bytree': 0.5411697855746465, 'min_child_weight': 9, 'gamma': 2.5969950442577985}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:09,606] Trial 18 finished with value: 0.8186333333333333 and parameters: {'max_depth': 5, 'learning_rate': 0.0019858983394202942, 'n_estimators': 99, 'subsample': 0.5755393818774721, 'colsample_bytree': 0.5118938225908063, 'min_child_weight': 7, 'gamma': 0.6279900491128196}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:10,681] Trial 19 finished with value: 0.8572 and parameters: {'max_depth': 7, 'learning_rate': 0.00499192216324444, 'n_estimators': 181, 'subsample': 0.5995184869901999, 'colsample_bytree': 0.6528936708104867, 'min_child_weight': 10, 'gamma': 1.5593627996899397}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:11,311] Trial 20 finished with value: 0.5281666666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.00029641469555844423, 'n_estimators': 92, 'subsample': 0.659631650504467, 'colsample_bytree': 0.7866088061801103, 'min_child_weight': 8, 'gamma': 3.1622388965512993}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:12,283] Trial 21 finished with value: 0.8541666666666666 and parameters: {'max_depth': 6, 'learning_rate': 0.007419099964775183, 'n_estimators': 181, 'subsample': 0.6742938794814548, 'colsample_bytree': 0.8994121582127581, 'min_child_weight': 10, 'gamma': 0.3794411156690765}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:13,263] Trial 22 finished with value: 0.8617000000000001 and parameters: {'max_depth': 6, 'learning_rate': 0.009794156846064769, 'n_estimators': 189, 'subsample': 0.6143842541844895, 'colsample_bytree': 0.8240672028494521, 'min_child_weight': 9, 'gamma': 1.2088904774131435}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:14,187] Trial 23 finished with value: 0.8667666666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.005962600768275822, 'n_estimators': 151, 'subsample': 0.7345266107421169, 'colsample_bytree': 0.8235182271469293, 'min_child_weight': 9, 'gamma': 1.2055740352635707}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:14,961] Trial 24 finished with value: 0.8493666666666666 and parameters: {'max_depth': 6, 'learning_rate': 0.003842667577573059, 'n_estimators': 147, 'subsample': 0.7334306476769183, 'colsample_bytree': 0.8117981422784448, 'min_child_weight': 9, 'gamma': 2.169815341504963}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:15,934] Trial 25 finished with value: 0.8698 and parameters: {'max_depth': 7, 'learning_rate': 0.006081946851098921, 'n_estimators': 157, 'subsample': 0.7679056277294012, 'colsample_bytree': 0.7574242306414415, 'min_child_weight': 8, 'gamma': 1.2530347025221622}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:16,920] Trial 26 finished with value: 0.8569666666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.0016749886272191, 'n_estimators': 157, 'subsample': 0.7840307535339059, 'colsample_bytree': 0.7479085741355318, 'min_child_weight': 8, 'gamma': 2.016380909550268}. Best is trial 1 with value: 0.8702666666666667.\n",
      "[I 2024-07-24 22:11:17,855] Trial 27 finished with value: 0.8707666666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.006498635150556186, 'n_estimators': 136, 'subsample': 0.8523754645721504, 'colsample_bytree': 0.6778027984960993, 'min_child_weight': 6, 'gamma': 1.6713321365144433}. Best is trial 27 with value: 0.8707666666666667.\n",
      "[I 2024-07-24 22:11:18,319] Trial 28 finished with value: 0.8051999999999999 and parameters: {'max_depth': 3, 'learning_rate': 0.003996830441423231, 'n_estimators': 135, 'subsample': 0.8938075074121428, 'colsample_bytree': 0.6558584047097773, 'min_child_weight': 6, 'gamma': 2.920353080561619}. Best is trial 27 with value: 0.8707666666666667.\n",
      "[I 2024-07-24 22:11:19,336] Trial 29 finished with value: 0.8648333333333333 and parameters: {'max_depth': 7, 'learning_rate': 0.002464107190223664, 'n_estimators': 167, 'subsample': 0.8464452615153435, 'colsample_bytree': 0.6792521533068747, 'min_child_weight': 5, 'gamma': 1.842321632809912}. Best is trial 27 with value: 0.8707666666666667.\n",
      "[I 2024-07-24 22:11:20,149] Trial 30 finished with value: 0.8711333333333334 and parameters: {'max_depth': 7, 'learning_rate': 0.006821649195205967, 'n_estimators': 117, 'subsample': 0.8597297786316136, 'colsample_bytree': 0.7388133894741667, 'min_child_weight': 5, 'gamma': 2.348962556438248}. Best is trial 30 with value: 0.8711333333333334.\n",
      "[I 2024-07-24 22:11:20,923] Trial 31 finished with value: 0.8703 and parameters: {'max_depth': 7, 'learning_rate': 0.007051869387559122, 'n_estimators': 117, 'subsample': 0.8614948764856083, 'colsample_bytree': 0.7465205255478209, 'min_child_weight': 5, 'gamma': 2.3502207486466378}. Best is trial 30 with value: 0.8711333333333334.\n",
      "[I 2024-07-24 22:11:21,600] Trial 32 finished with value: 0.8704333333333333 and parameters: {'max_depth': 7, 'learning_rate': 0.007444194645385193, 'n_estimators': 108, 'subsample': 0.8562713106002808, 'colsample_bytree': 0.7336496457879571, 'min_child_weight': 5, 'gamma': 2.3647915823537207}. Best is trial 30 with value: 0.8711333333333334.\n",
      "[I 2024-07-24 22:11:22,326] Trial 33 finished with value: 0.8711 and parameters: {'max_depth': 7, 'learning_rate': 0.007280138039304127, 'n_estimators': 112, 'subsample': 0.8561623533995397, 'colsample_bytree': 0.7378311679382029, 'min_child_weight': 5, 'gamma': 2.461425612965704}. Best is trial 30 with value: 0.8711333333333334.\n",
      "[I 2024-07-24 22:11:22,986] Trial 34 finished with value: 0.8665666666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.004301358678000349, 'n_estimators': 103, 'subsample': 0.8128440019512535, 'colsample_bytree': 0.6904654025560961, 'min_child_weight': 4, 'gamma': 2.896964805404491}. Best is trial 30 with value: 0.8711333333333334.\n",
      "[I 2024-07-24 22:11:23,540] Trial 35 finished with value: 0.8676 and parameters: {'max_depth': 7, 'learning_rate': 0.007035839076921824, 'n_estimators': 84, 'subsample': 0.8421071166304449, 'colsample_bytree': 0.7314429669326868, 'min_child_weight': 5, 'gamma': 1.7365945075569837}. Best is trial 30 with value: 0.8711333333333334.\n",
      "[I 2024-07-24 22:11:24,161] Trial 36 finished with value: 0.8498666666666667 and parameters: {'max_depth': 6, 'learning_rate': 0.0030688164963437623, 'n_estimators': 112, 'subsample': 0.8715492484444411, 'colsample_bytree': 0.7931931366874106, 'min_child_weight': 6, 'gamma': 2.354688785912769}. Best is trial 30 with value: 0.8711333333333334.\n",
      "[I 2024-07-24 22:11:25,043] Trial 37 finished with value: 0.8752333333333333 and parameters: {'max_depth': 7, 'learning_rate': 0.009977117486517376, 'n_estimators': 132, 'subsample': 0.7972804210595148, 'colsample_bytree': 0.6744028682987737, 'min_child_weight': 4, 'gamma': 3.561321902050875}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:25,665] Trial 38 finished with value: 0.8362999999999999 and parameters: {'max_depth': 5, 'learning_rate': 0.004366559445282647, 'n_estimators': 130, 'subsample': 0.7927564925062965, 'colsample_bytree': 0.6113743896675088, 'min_child_weight': 4, 'gamma': 3.5924910423435317}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:26,401] Trial 39 finished with value: 0.8587666666666667 and parameters: {'max_depth': 6, 'learning_rate': 0.008928249508589826, 'n_estimators': 136, 'subsample': 0.8336619552119935, 'colsample_bytree': 0.6740851215046904, 'min_child_weight': 4, 'gamma': 4.936401281912033}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:27,159] Trial 40 finished with value: 0.7511 and parameters: {'max_depth': 7, 'learning_rate': 0.0008099689302725361, 'n_estimators': 121, 'subsample': 0.8999218981950167, 'colsample_bytree': 0.619998880414386, 'min_child_weight': 6, 'gamma': 3.7131520860601057}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:27,886] Trial 41 finished with value: 0.8693666666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.006753430416067255, 'n_estimators': 107, 'subsample': 0.8601114173548959, 'colsample_bytree': 0.723096763312282, 'min_child_weight': 5, 'gamma': 2.728851089057423}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:28,528] Trial 42 finished with value: 0.8685333333333333 and parameters: {'max_depth': 7, 'learning_rate': 0.00773649733818485, 'n_estimators': 94, 'subsample': 0.8056928920269103, 'colsample_bytree': 0.6998822525423982, 'min_child_weight': 5, 'gamma': 3.14330471311402}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:29,340] Trial 43 finished with value: 0.8727 and parameters: {'max_depth': 7, 'learning_rate': 0.009374159396009328, 'n_estimators': 127, 'subsample': 0.814794351808922, 'colsample_bytree': 0.7181159394367357, 'min_child_weight': 4, 'gamma': 4.3754593392214876}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:30,138] Trial 44 finished with value: 0.8732666666666666 and parameters: {'max_depth': 7, 'learning_rate': 0.00999058452823282, 'n_estimators': 126, 'subsample': 0.7623703055433406, 'colsample_bytree': 0.6746395477947017, 'min_child_weight': 4, 'gamma': 4.183698989477224}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:30,973] Trial 45 finished with value: 0.8575666666666667 and parameters: {'max_depth': 6, 'learning_rate': 0.00858779139241614, 'n_estimators': 125, 'subsample': 0.7532646190324507, 'colsample_bytree': 0.7089692389397011, 'min_child_weight': 4, 'gamma': 4.299612187212887}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:31,737] Trial 46 finished with value: 0.5281666666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.00015991830242234237, 'n_estimators': 118, 'subsample': 0.8167612712002569, 'colsample_bytree': 0.6284553321010984, 'min_child_weight': 4, 'gamma': 4.489180061549184}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:32,428] Trial 47 finished with value: 0.8581666666666667 and parameters: {'max_depth': 6, 'learning_rate': 0.008933802599960208, 'n_estimators': 129, 'subsample': 0.7614465034498774, 'colsample_bytree': 0.7198942803360893, 'min_child_weight': 4, 'gamma': 3.9668722696433503}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:32,942] Trial 48 finished with value: 0.8593333333333334 and parameters: {'max_depth': 7, 'learning_rate': 0.009851669165009612, 'n_estimators': 78, 'subsample': 0.7170581648027583, 'colsample_bytree': 0.5739272607502386, 'min_child_weight': 4, 'gamma': 4.754019427193612}. Best is trial 37 with value: 0.8752333333333333.\n",
      "[I 2024-07-24 22:11:33,673] Trial 49 finished with value: 0.8681333333333333 and parameters: {'max_depth': 7, 'learning_rate': 0.0049719728795835725, 'n_estimators': 112, 'subsample': 0.7978031417581632, 'colsample_bytree': 0.6916048000144916, 'min_child_weight': 4, 'gamma': 4.200062263271995}. Best is trial 37 with value: 0.8752333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "0.8752333333333333\n",
      "{'max_depth': 7, 'learning_rate': 0.009977117486517376, 'n_estimators': 132, 'subsample': 0.7972804210595148, 'colsample_bytree': 0.6744028682987737, 'min_child_weight': 4, 'gamma': 3.561321902050875}\n"
     ]
    }
   ],
   "source": [
    "# xgboostclassifier\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def xgb_objective(trial):\n",
    "    # 하이퍼파라미터 범위 설정\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 7)  # max_depth의 범위를 줄임\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)  # n_estimators의 범위를 줄임\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 0.9)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 0.9)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 4, 10)  # 추가\n",
    "    gamma = trial.suggest_float('gamma', 0, 5)  # 추가\n",
    "    \n",
    "    # XGBClassifier 모델 정의\n",
    "    clf = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        min_child_weight=min_child_weight,  # 추가\n",
    "        gamma=gamma,  # 추가\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    # 교차 검증 점수 계산\n",
    "    scores = cross_val_score(clf, shap_xgb_X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Optuna 스터디 생성 및 최적화\n",
    "xgb_study = optuna.create_study(direction='maximize')\n",
    "xgb_study.optimize(xgb_objective, n_trials=50)  # 최적화 반복 횟수는 필요에 따라 조절\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "xgb_best_params = xgb_study.best_params\n",
    "print(' ')\n",
    "print(xgb_study.best_value)\n",
    "print(xgb_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8248175182481752\n"
     ]
    }
   ],
   "source": [
    "model_logis=xgb.XGBClassifier(**xgb_best_params)\n",
    "model_logis.fit(shap_xgb_X_train_resampled, y_train_resampled)\n",
    "y_pred = model_logis.predict(shap_xgb_X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy=\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LogisticRegression\n",
    "# def logreg_objective(trial):\n",
    "\n",
    "#     r = trial.suggest_float('l1_ratio', 0.3, 0.8, log=False)  # 범위를 0.1에서 0.9로 좁힘\n",
    "#     c = trial.suggest_float('C', 1e-4, 1e2, log=True)\n",
    "#     max_iter = trial.suggest_int('max_iter', 500, 2000, step=500)  # max_iter 튜닝 추가\n",
    "    \n",
    "#     clf =  LogisticRegression(max_iter=max_iter, solver='saga', penalty='elasticnet', l1_ratio=r, C=c)\n",
    "#     scores = cross_val_score(clf, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "    \n",
    "#     return scores.mean()\n",
    "    \n",
    "# logreg_study = optuna.create_study(direction='maximize')\n",
    "# logreg_study.optimize(logreg_objective, n_trials=20)\n",
    "\n",
    "# logreg_best_params = logreg_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_logis=LogisticRegression(**logreg_best_params)\n",
    "# model_logis.fit(X_train_resampled,y_train_resampled)\n",
    "# y_pred = model_logis.predict(X_val)\n",
    "\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import RidgeClassifier\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# # Ridge Classifier의 목적 함수\n",
    "# def ridge_classifier_objective(trial):\n",
    "\n",
    "#     alpha = trial.suggest_float('alpha', 1e-5, 1e3, log=True)\n",
    "#     clf = RidgeClassifier(alpha=alpha)\n",
    "    \n",
    "#     # 교차 검증을 통한 모델 평가 (AUC 스코어)\n",
    "#     # 다중 클래스의 경우, 'ovr' 또는 'ovo' 스키마를 사용\n",
    "#     # if len(set(y_train_resampled)) > 2:\n",
    "#     #     scoring = 'roc_auc_ovr'\n",
    "#     # else:\n",
    "#     #     scoring = 'roc_auc'\n",
    "    \n",
    "#     scores = cross_val_score(clf, shap_xgb_X_train_resampled, y_train_resampled, cv=6, scoring='roc_auc')\n",
    "#     return scores.mean()\n",
    "\n",
    "# ridge_classifier_study = optuna.create_study(direction='maximize')\n",
    "# ridge_classifier_study.optimize(ridge_classifier_objective, n_trials=200)\n",
    "# ridge_classifier_best_params = ridge_classifier_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 최적의 하이퍼파라미터로 최종 모델 학습 및 평가\n",
    "# # model_ridge_classifier = RidgeClassifier(**ridge_classifier_best_params)\n",
    "# model_ridge_classifier = RidgeClassifier()\n",
    "# model_ridge_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# # 검증 데이터로 예측\n",
    "# y_pred = model_ridge_classifier.predict(X_val)\n",
    "\n",
    "# # 정확도 계산\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 최적의 하이퍼파라미터로 최종 모델 학습 및 평가\n",
    "# model_ridge_classifier = RidgeClassifier(**ridge_classifier_best_params)\n",
    "# model_ridge_classifier.fit(shap_xgb_X_train_resampled, y_train_resampled)\n",
    "\n",
    "# # 검증 데이터로 예측\n",
    "# y_pred = model_ridge_classifier.predict(shap_xgb_X_val)\n",
    "# y_pred_proba = model_ridge_classifier.decision_function(shap_xgb_X_val)\n",
    "\n",
    "# # 다중 클래스의 경우 ROC AUC 스코어 계산\n",
    "# if len(set(y_val)) > 2:\n",
    "#     y_val_bin = label_binarize(y_val, classes=list(set(y_val)))\n",
    "#     auc_score = roc_auc_score(y_val_bin, y_pred_proba, multi_class='ovr')\n",
    "# else:\n",
    "#     auc_score = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "# print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logis=LogisticRegression(l1_ratio= 0.789370019111903, C= 0.07701480047825814, max_iter= 1000)\n",
    "model_logis.fit(X_train_resampled,y_train_resampled)\n",
    "y_pred = model_logis.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomforest feature selection  큰 효과가 없는 듯 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier()\n",
    "# model.fit(train_resampled, y_train_resampled)\n",
    "# y_pred = model.predict(test)\n",
    "\n",
    "# rn_features = []\n",
    "# importances = model.feature_importances_\n",
    "# feature_names = train.columns\n",
    "\n",
    "# # 피처 중요도를 기준으로 정렬하여 상위 피처 선택\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# # 중요도가 0.01 이상인 피처만 선택\n",
    "# # top_number = 40\n",
    "# top_num_indices = [idx for idx in indices if importances[idx] >= 0.005] #[:top_number]\n",
    "# top_features = feature_names[top_num_indices]\n",
    "\n",
    "# for i, feature in enumerate(top_features):\n",
    "#     print(f\"{i+1}. {feature} (중요도: {importances[top_num_indices[i]]})\")\n",
    "#     rn_features.append(feature)\n",
    "\n",
    "# rn_train_resampled = train_resampled[rn_features]\n",
    "# rn_test = test[rn_features]\n",
    "\n",
    "# model_logis=LogisticRegression(**logreg_best_params)\n",
    "# model_logis.fit(rn_train_resampled,y_train_resampled)\n",
    "# y_pred = model_logis.predict(rn_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상관계수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X_train.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "corr_number = 0.9\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > corr_number)]\n",
    "to_drop\n",
    "# # 특징 제거\n",
    "corr_X_train_resampled = X_train_resampled.drop(columns=to_drop)  \n",
    "corr_X_val = X_val.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_objective(trial):\n",
    "    \n",
    "    r = trial.suggest_float('l1_ratio', 0, 1, log=False)\n",
    "    c = trial.suggest_float('C', 1e-4, 1e2, log=True)\n",
    "     \n",
    "    clf =  LogisticRegression(max_iter=5000, solver='saga', penalty='elasticnet', l1_ratio=r, C=c)\n",
    "    scores = cross_val_score(clf, corr_X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return scores.mean()\n",
    "    \n",
    "logreg_study = optuna.create_study(direction='maximize')\n",
    "logreg_study.optimize(logreg_objective, n_trials=5)\n",
    "\n",
    "logreg_best_params = logreg_study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logis=LogisticRegression(**logreg_best_params)\n",
    "model_logis.fit(corr_X_train_resampled,y_train_resampled)\n",
    "y_pred = model_logis.predict(corr_X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 규제(Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha 값 후보군 설정\n",
    "alpha_values = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 1, 10, 100]\n",
    "param_grid = {'alpha': alpha_values}\n",
    "\n",
    "# Lasso 모델과 GridSearchCV 설정\n",
    "lasso = Lasso()\n",
    "grid_search = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# 최적의 alpha 값 찾기\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=best_alpha)  # 위에서 나온 alpha 값으로 조정한 거임\n",
    "lasso.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 가중치가 0이 아닌 특징 선택\n",
    "selected_features = X_train_resampled.columns[lasso.coef_ != 0]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_X_train_resampled = X_train_resampled[selected_features]\n",
    "L1_X_val = X_val[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_objective(trial):\n",
    "    \n",
    "    r = trial.suggest_float('l1_ratio', 0, 1, log=False)\n",
    "    c = trial.suggest_float('C', 1e-4, 1e2, log=True)\n",
    "     \n",
    "    clf =  LogisticRegression(max_iter=5000, solver='saga', penalty='elasticnet', l1_ratio=r, C=c)\n",
    "    scores = cross_val_score(clf, L1_X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return scores.mean()\n",
    "    \n",
    "logreg_study = optuna.create_study(direction='maximize')\n",
    "logreg_study.optimize(logreg_objective, n_trials=5)\n",
    "\n",
    "logreg_best_params = logreg_study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2013~2023  \n",
    "### 많긴 하지만 정확도 떨어질 것으로 예상됨\n",
    "\n",
    "# 2020~2023\n",
    "### 데이터 수가 급격히 줄어듬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
